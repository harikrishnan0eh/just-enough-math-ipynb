{
  "metadata": {
    "name": "Show Me The Monoid"
  },
  "nbformat": 3,
  "nbformat_minor": 0,
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Show Me The Monoid"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "**Topics:** _Abstract Algebra_, _Category Theory_, _Functional Programming_, _Data Workflows_, _Parallel Processing_, _Combinatorics_"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The 1996 film [Jerry Maguire](http://en.wikipedia.org/wiki/Jerry_Maguire \"\") featured a famous line, “Show me the money!” Many advanced machine learning practices that run on Big Data leverage a variant of the Hollywood line from that movie: “Show me the monoid!” Why? Because there’s lots of number-crunching to perform, and parallel processing is generally a good way to avoid having your number-crunching job take days/weeks/months to run. However, parallel processing is hard and the “popular” techniques have tended to be obscure and require sophisticated programming and even more sophisticated and expensive operations. Fortunately a new class of open source technology -- led by Twitter  , eBay, LinkedIn, and some other firms -- has made parallel processing much simpler and less expensive. You’ll need to understand a bit of abstract algebra to make sense of the functional programming that gets used. That in turn will help you build high-ROI apps at scale."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "[ ![Show Me The Monoid](http://delimiter.com.au/wp-content/uploads/2011/03/jerrymaguire.jpg) ](http://en.wikipedia.org/wiki/Jerry_Maguire \"\")"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For example, if you are working with MapReduce   programs – that is, if you are working with efficient MapReduce programs at scale, running as high-ROI apps – then your code probably leverages a couple of important notions implicitly (1) a _monoid  _ pattern, and (2) a _barrier_ pattern. Your code may not acknowledge that fact, and quite frankly the engineers on your team may not even know about these or be able to describe them. However, MapReduce (e.g.,  [Apache Hadoop](http://hadoop.apache.org/ \"\")) is a simplified combination of those two patterns -- then implemented as a fault-tolerant batch scheduler atop clusters of commodity hardware."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "By understanding the design patterns and the underlying math we can explore beyond Hadoop, and begin to look at more advanced approaches to machine learning at scale for production use."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The notion of _monads_ traces back in Greek philosophy (or earlier) to Pythagoras. That was revisited in great detail by Renaissance mathematicians such as [John Dee](http://www.esotericarchives.com/dee/monad.htm \"\"), Giordano Bruno, Gottfried Leibniz, et al. More contemporary usage of the term _monoid_ was introduced by [Claude Chevalley](http://books.google.com/books?id=vv7zkx_cSqYC&printsec=frontcover&source=gbs_atb#v=onepage&q&f=false \"\") in 1956 as “an internal law of composition”, a notion which subsequently migrated into computer science. The big idea here is that we want to be able to compute functions using really large-scale data. The kinds of functions we want to compute follow the old, familiar form of:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\ny = f(x)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Simple enough. Based on the functions we define, we need to build and run Enterprise data workflows – employing highly robust approaches, not just twiddling bits here. Solid engineering. Envision requirements that involve lots of data sources, lots of use cases, lots of functions to compute. The math looks more like:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\ny = f(x)\nv = g(y)\nu = h(v)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "… And so on. Since we have some complicated data workflows   to define, those functions end up being stated as _dependency graphs_, where one computed data product flows into another. Consequently, we’ll see lots of code in which the functions get _composed  _, along the lines of:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nu = h(g(f(x)))\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Great. That should be simple to compute, right? However – and it becomes quite a big honking **HOWEVER** in high-ROI apps at scale – this is not particularly simple to compute when the data grows large and the functions become complex."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s consider a use case. Imagine that you run Twitter  . You need to compute followers to recommend, where to place sponsored tweets, etc. You have hundreds of millions of users, each of which probably has maybe a few hundred followers. So you could, for example, write code on your laptop to analyze the tweets for each user, leverage [Cosine Similarity](http://pyevolve.sourceforge.net/wordpress/?p=2497) to compare interests between users, then make recommendations among their followers…"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Ka’chunk, ka’chunk, your laptop computer crunches through each user’s data. Operations get repeated billions of times on different data. That’ll take a while to process. Moreover, some users – Taylor Swift, Lady Gaga, Barak Obama, Katy Perry, et al., – have _millions_ of followers. Your laptop keeps running out of memory when processing recommendations for those. Or worse, it crashes in the midst of one of the “heavy hitters  ”, so you must repeat costly work."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Alternatively, thousands of computers could each crunch through thousands of users. That’d be quicker to reach actionable results, a.k.a. revenue. However, any one of those `v = g(y)` functions could become a problem since the data for a “heavy hitter” might not fit on a single computer. Moreover, we really want to be computing more complex kinds of workflows, along the lines of `u = h(g(f(x)))` instead of just one-offs. Your blood pressure rises. You can just imagine the escalating datacenter invoices, engineering schedule delays, angry users complaining in public, plummeting revenue reports, journalists having a feeding frenzy at your expense … let alone the next board meeting with investors."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Relax, we can leverage abstract algebra to the rescue. There are mathematically rigorous definitions for how to split the data into chunks that our servers can handle, apply workflows of composable functions in parallel, and reassemble the results into actionable insights. Calling back to the [Maguire film](http://en.wikipedia.org/wiki/Jerry_Maguire), you had me at “workflows of composable functions.”"
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Definitions"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s start with some basic definitions for abstract algebra, from [Introduction to Semigroups and Monoids](http://math.uga.edu/~pete/semigroup.pdf) by Pete L. Clark  . To paraphrase one of the first definitions given in that paper:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Okay, that’s a mouthful – and brace yourself, terminology in this area of mathematics gets much, much worse. Imagine studying it for the first time from someone who spoke with a lisp in a very thick German accent. But I digress…"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The two main properties of a semigroup are _associativity  _ and _closure  _. One example would be to consider **N**, the set of _natural numbers_ (including zero), along with `+` the addition operator. The operation is _binary_ and _associative_:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\n2 + (3 + 4) = (2 + 3) + 4 \n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Keep in mind that the results of the operation must also be within set **N**. That defines the required property for _closure_."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Actually, using integers for examples here is not ideal. Integers have a whole bunch of other properties, which we really don’t want to confuse here. But that works for now."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Next, let’s consider the second definition in the [Clark paper](http://math.uga.edu/~pete/semigroup.pdf):"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Bokay. Well, the number `0` works pretty well as an _identity element  _ for addition:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\n2 + 0 = 0 + 2 = 2\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Skipping a bit further into the [Clark paper](http://math.uga.edu/~pete/semigroup.pdf) at the bottom of page 9, we find:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We could go further, exploring concise definitions for _groups_, _fields_, etc. But let’s not – just yet."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Note that when we’re talking about _elements_ in the context of semigroups, monoids, groups, rings, etc., these could be integer numbers… or they could be vectors  , matrices  , or tensors… or polynomials… while we’re on a roll here, how about if they were [circulants](http://www.slideshare.net/dgleich/the-power-and-arnoldi-methods-in-an-algebra-of-circulants)? Or how about if they were chunks of code? Or instances of classes in an object-oriented programming language?"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Looking at this from a slightly different perspective, we know from arithmetic that `2 + 3 = 5` , and we know from algebra that when `2 + x = 5` then `x = 3` … great, that comes in handy. We can perhaps solve for a variable and find its correct value. However, imagine if the variable was not representing a number? What if the variable represented the operation used in the equation? Or what if the variable represented the entire set of numbers allowed in the equation? That’s quite abstract. Yes, exactly, so we call it  _abstract algebra_ for a good reason. In general, this area is called [Category Theory](http://en.wikipedia.org/wiki/Category_theory \"\") – in case you feel an urge to get really abstract."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Why bother with this? Stated simply, it allows us to write very generic functions. More to the point, we need to leverage the algebraic properties of those generic functions to build and optimize data workflows at scale, maximizing for parallelism and ROI, minimizing latency, while also minimizing maintenance costs."
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Our Old Friend, MapReduce"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s take a look at [Monoidify! Monoids as a Design Principle for Efficient MapReduce Algorithms](http://arxiv.org/pdf/1304.7544v1.pdf \"\") by [Jimmy Lin](http://www.umiacs.umd.edu/~jimmylin/ \"\"), U Maryland/Twitter  . Kudos to Oscar Boykin, Sam Ritchie, et al."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We’ll begin with **Algorithm 1**. Consider the following simple examples written in Python… First, let’s look at the code in `avg_m1.py` for the mapper:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nimport sys\n\nclass Mapper (object):\n    def map (self, key, val):\n        print \"%s\\t%d\" % ( key, int(val),  )\n\nm = Mapper()\n\nfor line in sys.stdin:\n    key, val = line.strip().split(\"\\t\")\n    m.map(key, val)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Next, let’s look at the code in `avg_r1.py` for the reducer:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nimport sys\n\nclass Reducer (object):\n    def reduce (self, key, values):\n        self.total = 0\n        self.count = 0\n\n        for val in values:\n            self.total += int(val)\n            self.count += 1\n\n        avg = self.total / float(self.count)\n        print \"%s\\t%f\" % ( key, avg, )\n\nlast_key = None\n\nfor line in sys.stdin:\n    key, val = line.strip().split(\"\\t\")\n\n    if not last_key == key:\n        if last_key:\n            r.reduce(last_key, values)\n\n        r = Reducer()\n        values = []\n\n    values.append(val)\n    last_key = key\n\nr.reduce(key, values)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Given the data in `num.txt` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\na   3\nc   9\na   7\nd   2\nb   2\na   1\nb   4\nc   1\nd   8\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s run that data through our MapReduce pipeline to calculate an average of the values for each key:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\n$ cat num.txt | python avg_m1.py | sort | python avg_r1.py \na   3.666667\nb   3.000000\nc   5.000000\nd   5.000000\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Effectively, that’s a Python implementation of **Algorithm 1** in the [Lin paper](http://arxiv.org/pdf/1304.7544v1.pdf). We could have used [Hadoop streaming](http://wiki.apache.org/hadoop/HadoopStreaming \"\") with the same code and same data, to get the same results. For that matter, we might as well have used SQL to express this approach:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nSELECT\n  key, AVERAGE(val)\nFROM num\nGROUP BY key\n;\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But we don’t want to use SQL, because it won’t scale very well. Recall the issues with Lady Gaga’s data on Twitter: SQL is a really, _truly_ excellent way to get into trouble quickly with large-scale data. Even in an tiny, early stage tech start-up, similar kinds of data issues can become show-stoppers for attempts to leverage machine learning."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If one uses Hadoop streaming, and one writes code like the Python code shown above, one will almost surely want to improve its performance. In other words, that kind of code runs as slow as molasses. Could we improve it by making it use of [combiners](http://developer.yahoo.com/hadoop/tutorial/module4.html#functionality)? Or something."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Lin makes the point:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "…then delivers the punchline about monoids and parallelism:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s skip down in the [Lin paper](http://arxiv.org/pdf/1304.7544v1.pdf) to **Algorithm 3** and **Algorithm 4**. The code in `avg_m4.py` presents an improved mapper based on that approach:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nfrom collections import defaultdict\nimport sys\n\nclass Mapper (object):\n    def __init__ (self):\n        self.total = defaultdict(lambda: 0, {})\n        self.count = defaultdict(lambda: 0, {})\n\n    def map (self, key, val):\n        self.total[key] += int(val)\n        self.count[key] += 1\n\n    def close (self):\n        for key, val in self.total.items():\n            print \"%s\\t%d\\t%d\" % ( key, val, self.count[key], )\n\nm = Mapper()\n\nfor line in sys.stdin:\n    key, val = line.strip().split(\"\\t\")\n    m.map(key, val)\n\nm.close()\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The code in `avg_r4.py` presents an improved reducer based on **Algorithm 3** in the [Lin paper](http://arxiv.org/pdf/1304.7544v1.pdf):"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nimport sys\n\nclass Reducer (object):\n    def reduce (self, key, values):\n        self.total = 0\n        self.count = 0\n\n        for _total, _count in values:\n            self.total += int(_total)\n            self.count += int(_count)\n\n        avg = self.total / float(self.count)\n        print \"%s\\t%f\" % ( key, avg, )\n\nlast_key = None\n\nfor line in sys.stdin:\n    vals = line.strip().split(\"\\t\")\n    key = vals.pop(0)\n\n    if not last_key == key:\n        if last_key:\n            r.reduce(last_key, values)\n\n        r = Reducer()\n        values = []\n\n    values.append(vals)\n    last_key = key\n\nr.reduce(key, values)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In other words, there’s no need to define a combiner, since the mapper and reducer handle intermediate values in a much better way. Lin summarizes:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Can haz monoid? Yes, can haz!"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Jimmy Lin goes into much more detail about the use of monoids for designing MapReduce algorithms, for compiler optimization, etc. Highly recommended. In particular, he mentions about _stochastic gradient descent  _ used to train classifiers, with the model parameter as a monoid – please keep that in mind for later! In general, he references about the [Algebird](https://github.com/twitter/algebird \"\") open source project from Twitter  , used by [Scalding](https://github.com/twitter/scalding \"\"), [Spark](http://spark.incubator.apache.org/ \"\"), [Summingbird](https://github.com/twitter/summingbird \"\"), etc. We’ll come back to Algebird in a moment."
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Curried Lambda Stew"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "As all good Star Trek fans know, Data is intended to be _fully functional_. Digging into the origins of _functional programming  _, we find plenty of references to [Haskell Curry](http://en.wikipedia.org/wiki/Haskell_Curry \"\"), known for seminal work on [combinatory logic](http://www.haskell.org/haskellwiki/Combinatory_logic) (1927), and to [Alonzo Church](http://en.wikipedia.org/wiki/Alonzo_Church \"\"), known for [lambda calculus](http://plato.stanford.edu/entries/lambda-calculus/ \"\") (1936) and much more! Both of those formulations sought to define formal answers to the question “What can be computed?” The work of Curry and Church, respectively, helped inform a wide range computer science research, and gave rise to programming languages such as [LISP](http://xkcd.com/224/)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "[ ![Data](http://img.youtube.com/vi/bHcN4Gm8tzM/0.jpg) ](http://youtu.be/bHcN4Gm8tzM)"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Rolling the clock out to 1977, John Backus   incorporated those notions and more into a relatively “pure” form of functional programming, presented in the award-winning paper, [Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs](http://www.stanford.edu/class/cs242/readings/backus.pdf). On page 619, we find a key point articulated quite clearly:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Backus was quite interested in, among other things, how to increase the parallelism of programs, moving away from [Von Neumann architecture](http://en.wikipedia.org/wiki/Von_Neumann_architecture \"\"). He summarized four key elements of his approach:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>a functional style of programming without variables</li>\n\t<li>an algebra of functional programs</li>\n\t<li>a formal functional programming system</li>\n\t<li>applicative state transition<a contenteditable=\"false\" data-primary=\"applicative state transition\" data-type=\"indexterm\" id=\"idm186544\"> </a> systems</li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Then skipping to page 638:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Good stuff, straight to our point from the examples discussed above, and well worth a careful reading. However, it’s not immediately useful in practice since you won’t find many FP-based open source projects on GitHub."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Speaking of applicatives… A decade or so later in 1990, other research efforts converged on an open standard for functional programming called [Haskell](http://www.haskell.org/haskellwiki/Haskell). Arguably, this language put into practice much of what Backus had envisioned. Let’s take a good look at a recommended text, [Learn You a Haskell for Great Good!](http://learnyouahaskell.com/functors-applicative-functors-and-monoids) by Miran Lipovača  ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In contrast to our Python examples, Haskell shows how a strongly typed language approaches these algebraic abstractions. The type system itself is where the math shows up most directly. Lipovača starts out with type classes like functors and applicatives, then moves into monoids  . Again, the definition is an associative, binary operation on a set, with an identity element  :"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>the function takes two parameters</li>\n\t<li>the parameters and the returned value have the same type</li>\n\t<li>there exists such a value that doesn’t change other values when used with the binary function</li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Continuing on, the [Lipovača article](http://learnyouahaskell.com/functors-applicative-functors-and-monoids \"\") discusses how to handle edge cases with `Maybe` … how to perform _maps_ and _folds  _… how to apply monoids for work with lists, trees, etc. Keep in mind that graphs can be represented as trees or sparse matrices, and that most real-world problems in large-scale data involve some kind of graphs. Since monoids provide robust, mathematical definitions for how to compute parts of the data in chunks and how to handle “empty” elements, they tend to handle large, sparse matrices   quite well."
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "The Aviary"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Lipovača’s examples in Haskell set the stage for how abstract algebra gets used in Scala, namely with [Algebird](https://github.com/twitter/algebird \"\"), [Summingbird](https://github.com/twitter/summingbird \"\"), etc. As it turns out, there’s been a long history of associating [birds and combinators](http://www.angelfire.com/tx4/cus/combinator/birds.html \"\"), tracing to [Raymond Smullyan](http://en.wikipedia.org/wiki/To_Mock_a_Mockingbird \"\"):"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Quite fitting, given how much Twitter has invested in this area."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Before we jump into Scala, first let’s take a look at another foundational component. Rolling the clock up to late 2007, Chris Wensel   introduced a Java API called [Cascading](http://www.cascading.org/), as a way to build Enterprise data workflows   atop Hadoop. Rather than write code directly using the MapReduce API in Hadoop, programmers could use aspects of functional programming languages directly within Java to define workflows."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Thanks to a [separation of concerns](http://en.wikipedia.org/wiki/Separation_of_concerns) between the business logic of workflows versus the nuts and bolts of Hadoop  , Cascading   leveraged – you guessed it – notions about functors, applicatives, monoids, etc. Those aspects of Cascading are not called out explicitly, but they allow it to apply optimization techniques from query engines, compilers, etc., and provide a library of very generic functions."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "[ ![Cascading workflow](http://www.cascading.org/files/2012/08/plumb4.png) ](https://github.com/Cascading/Impatient/tree/master/part4)"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Another aspect of Cascading which is not called out explicitly is its use of a  \n[barrier pattern](http://instiki.cs242.vazexqi.com/publications/barrier.pdf), as a synchronization method for parallel computing. Pipes get defined by composing functions, handling our case of `u = h(g(f(x)))` quite nicely. Some functions get applied to the pipes at special points, for example immediately following a `GroupBy` aggregation as shown in the diagram. That implies that all of the preceding tasks have completed. In other words, map tasks run in parallel _before_ the barrier, and the `GroupBy` which comes _after_ the barrier in turn relies on those results. More about that in a moment."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This brings us to back to Scala. Rolling the clock up to 2012, Twitter introduced [Scalding](https://github.com/twitter/scalding/wiki) a Scala API for Cascading. Twitter subsequently reworked their revenue apps based on Scalding and has been [evangelizing](https://engineering.twitter.com/opensource) its adoption – now deployed at eBay, LinkedIn, etc."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Another Scala library from Twitter is called [Algebird](https://engineering.twitter.com/opensource/projects/algebird) which provides [abstract algebra definitions](https://github.com/twitter/algebird/wiki/Abstract-algebra-definitions) for Scalding, Storm, Spark, etc. Its release sparked lots of discussion: why would Twitter be interested in abstract algebra? See some great answers on [Quora](http://www.quora.com/Twitter-1/What-is-Twitters-interest-in-abstract-algebra-with-algebird) and [StackExchange](http://cs.stackexchange.com/questions/9648/what-use-are-groups-monoids-and-rings-in-database-computations \"\"). Oscar Boykin  , one of the main authors of these Twitter projects, explains:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s take a quick look at one of the better articles about Algebird, written by [Michael Noll](http://www.michael-noll.com/blog/2013/12/02/twitter-algebird-monoid-monad-for-large-scala-data-analytics/). Noll was interested to explore Algebird, monoid, monads, etc., and wrote this tutorial – which reads a bit like a journal. He shows a great example, which references back to our Twitter use case mentioned above, contending with “heavy hitters”  :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\n// Let's have a popularity contest on Twitter.\n// The user with the most followers wins!\nval barackobama = TwitterUser(\"BarackObama\", 40267391)\nval katyperry = TwitterUser(\"katyperry\", 48013573)\nval ladygaga = TwitterUser(\"ladygaga\", 40756470)\nval miguno = TwitterUser(\"miguno\", 731) // I participate, too.  Olympic spirit!\nval taylorswift = TwitterUser(\"taylorswift13\", 37125055)\n\nval winner: Max[TwitterUser] = Max(barackobama) + Max(katyperry) + Max(ladygaga) + Max(miguno) + Max(taylorswift)\nassert(winner.get == katyperry)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Katy Perry wins, with `48,013,573` followers. This approach also hints at being able to distribute algorithms, especially for estimates. We’ll come back to that in another section."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Looking at the section titled “Monoids”, we see a familiar discussion about associativity, closure, identity element, fold, etc. Note how these definitions get implemented Scala by leveraging the type system. The source code on GitHub shows some good examples, e.g., where `Monoid` subclasses `Semigroup` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\ntrait Monoid[@specialized(Int,Long,Float,Double) T] extends Semigroup[T] {\n  def zero : T //additive identity\n  def isNonZero(v: T): Boolean = (v != zero)\n  def assertNotZero(v : T) {\n    if(!isNonZero(v)) {\n      throw new java.lang.IllegalArgumentException(\"argument should not be zero\")\n    }\n  }\n\n  def nonZeroOption(v : T): Option[T] = {\n    if (isNonZero(v)) {\n      Some(v)\n    }\n    else {\n      None\n    }\n  }\n\n  // Override this if there is a more efficient means to implement\n  def sum(vs: TraversableOnce[T]): T = sumOption(vs).getOrElse(zero)\n}\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Looking at the section titled “Monads” in the [Noll article](http://www.michael-noll.com/blog/2013/12/02/twitter-algebird-monoid-monad-for-large-scala-data-analytics/), we see _monads_ as a kind of structure used for defining chains of functions. In other words, our friend `u = h(g(f(x)))` gets used to define data workflows. Note that this way of specifying the business logic of a workflow is expressed quite explicitly in algebra, in contrast to Cascading. Also, it applies beyond Hadoop – that is, Scalding atop Cascading atop Hadoop. Much the same code can be used either in Scalding (batch) or in Storm (real-time)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "That leads us into a more recent Twitter project, released in 2013, called [Summingbird](https://github.com/twitter/summingbird/wiki). Wired magazine covered [a story](http://www.wired.com/wiredenterprise/2013/11/twitter-summingbird/) with two of its principal authors, Oscar Boykin and Sam Ritchie. The notion is to leverage the abstract algebra definitions for the business logic of data workflows, relatively independent of the distributed frameworks underneath:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Back to the note above about using a barrier pattern… Apache Hadoop provides a popular framework for large-scale computing. However, if you have Summingbird and Algebird defining your workflows, that opens up some alternatives."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Suppose that you have some “building block” with which you can readily assemble new kinds of distributed frameworks to provide barrier patterns, provide a distributed file system, manage tasks and services in a fault-tolerant way on commodity hardware, etc. That would obviate the need for Hadoop. Looking back again at the Twitter open source page, notice that the [Mesos project](https://engineering.twitter.com/opensource/projects/mesos) is listed alongside Summingbird. The needed features described above can be assembled in about 100 lines of Scala, using the Mesos building blocks. Compare that with over [1 million lines of source code](http://hortonworks.com/blog/reality-check-contributions-to-apache-hadoop/) contributed to Apache Hadoop since 2006, and growing. Words to the wise."
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Monoids in Python"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s shift back to Python for another coding example. Take a look at the article [Monoids in Python](http://fmota.eu/blog/monoids-in-python.html) by Francisco Mota. The definitions _associativity_, _closure_, and _identity element_ discussed there should be familiar by now."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Mota defines a monoid in Python similar to how Algebird does the same in Scala – minus Scala’s [type safety](http://typesafe.com/platform/tools/scala):"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nclass Monoid:\n    def __init__(self, null, lift, op):\n        self.null = null\n        self.lift = lift\n        self.op   = op\n\n    def __call__(self, *args):\n        result = self.null\n        for arg in args:\n            arg = self.lift(arg)\n            result = self.op(result, arg)\n        return result\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Leveraging this class, the `summ` monoid defines how to add a list of integers:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nsumm = Monoid(0, lambda x: x, lambda a,b: a+b)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To put this into use:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nsumm(10, 20, 30) == 10 + 20 + 30 == 60\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Adding numbers together, got it. Let’s try with a more complex data structure – dictionaries. The following code comes from [Exelixi](https://github.com/ceteri/exelixi/blob/master/src/monoids.py):"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\ndictm = Monoid({}, lambda x: x, lambda a,b: dict_op(a, b))\n\ndef dict_op (a, b):\n    for key, val in b.items():\n        if not key in a:\n            a[key] = val\n        else:\n            a[key] += val\n    return a\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Okay, let’s run through the numbers… Notice that `dictm` uses an empty dictionary `{}` for the identity element. Check! It uses `lambda x: x` for the “lift” method – no type conversion required there, closure is simple in this case. Check! Then it uses `dict_op(a, b)` as a helper method to perform the associative binary operation – adding elements into the dictionary. Check! Here’s an example usage:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\n>>> from monoids import dictm\n>>> x1 = { \"a\": 2, \"b\": 3 }\n>>> x2 = { \"b\": 2, \"c\": 7 }\n>>> print x1, x2\n{'a': 2, 'b': 3} {'c': 7, 'b': 2}\n>>> print dictm.fold([x1, x2])\n{'a': 2, 'c': 7, 'b': 5}\n>>> \n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Great, now we have a monoid for dictionaries – and therefore, a way to combine the results of distributed processing which get expressed as dictionaries. That’s one of the most commonly used data structures in Python. Hint: if you just _happened_ to have a distributed data structure in Python running across a cluster, and you just _happened_ to have a barrier pattern implemented to synchronize parallel tasks, then using `dictm` or other monoids would really help aggregate results. No Hadoop required."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We’ll come back to this later, when we consider more advanced data structures used for [streaming large amounts of data](http://blog.aggregateknowledge.com/2012/10/25/sketch-of-the-day-hyperloglog-cornerstone-of-a-big-data-infrastructure/)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Oh, and let’s not give the impression that this kind of work is limited to Scala and Python. There are plenty of other excellent frameworks based on other environments. For example, check out [MBrace: Cloud Computing with Monads](http://plosworkshop.org/2013/preprint/dzik.pdf) by Jan Dzik, et al., for F# and .NET on Windows:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Also, there is [Simmer](https://github.com/avibryant/simmer) for running aggregations based on Algebird as Linux/Unix command line utilities. No. Hadoop. Required."
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Key Points"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The following key points have been abstracted from the various papers and articles linked above:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>Abstract algebra combined with functional programming allow for efficient, reusable code.</li>\n\t<li>By exploiting semigroup structure, we can build systems that can be parallelized correctly without even knowing the operations involved.</li>\n\t<li>Monoids take advantage of sparsity, such as in sparse matrices where almost all of the values will be a zero in some monoid.</li>\n\t<li>Solve the systems problems once for any semigroup, monoid, group, ring, field, etc., then plug in any algorithm without having to consider about Hadoop, Storm, etc.</li>\n\t<li>Monoids are composable, and therefore great for defining the business logic of large-scale data workflows.</li>\n</ul>\n"
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Suggested Books"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_To Mock a Mockingbird: And Other Logic Puzzles_  \nby Raymond Smullyan (2000)  \n[http://amazon.com/dp/0192801422](http://amazon.com/dp/0192801422)"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_Programming Scala: Scalability = Functional Programming + Objects_  \nby Dean Wampler, Alex Payne (2009)  \n[http://amazon.com/dp/0596155956](http://amazon.com/dp/0596155956 \"\")"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_Learn You a Haskell for Great Good!_  \nby Miran Lipovača (2011)  \n[http://amazon.com/dp/1593272839](http://amazon.com/dp/1593272839)"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_Enterprise Data Workflows with Cascading_  \nby Paco Nathan (2013)  \n[http://amazon.com/dp/1449358721](http://amazon.com/dp/1449358721)"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_Functional Programming in Scala_  \nby Paul Chiusano, Runar Bjarnason (2014)  \n[http://amazon.com/dp/1617290653](http://amazon.com/dp/1617290653)"
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Exercises"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ol>\n<li>Write a monoid in Python for <em>set union</em>, and show it running.</li>\n\t<li>Can you point out any semigroups or monoids described in the <a href=\"http://www.stanford.edu/class/cs242/readings/backus.pdf\">Backus paper</a>?</li>\n</ol>"
        }
      ],
      "metadata": {
      }
    }
  ]
}