{
  "metadata": {
    "name": "Lies, Damn Lies, Statistics, and Bayesian Statistics"
  },
  "nbformat": 3,
  "nbformat_minor": 0,
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Lies, Damn Lies, Statistics, and Bayesian Statistics"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "**Topics:** _Algorithmic Modeling_, _Bayesian Statistics_, _Point Estimates_, _Regularization_"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The Latin phrase _ceteris paribus_ translates into English as “all other things being equal.” That notion gets invoked in decision making in terms of _isolation_. We might assume that other factors are fixed while considering a particular item, or that they change so slowly that they appear constant at any point, or that they will not be affected by the item being considered."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Some practices in the field of Statistics have historically approached data modeling problems in much the same way. Isolate a phenomenon within the data – enough to draw analogies to some known and rigorously proven mathematical function, generally based on a _probability distribution_. Next, claim that other factors are constant with respect to your hypothesis, or with respect to changes over time, or even with respect to your measuring process. Then draw conclusions. It’s somewhat appalling, but a vast number of ad-tech start-ups operate this way."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Back in the good ole days, when a computer might take up most of an entire room, we were taught to employ an approach called _data modeling_:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ol>\n<li>\n\t<p>prepare a sample of the data</p>\n\t</li>\n\t<li>\n\t<p>fit the sample to a <a href=\"http://en.wikipedia.org/wiki/Probability_density_function\">known distribution</a></p>\n\t</li>\n\t<li>\n\t<p>perform significance testing to show how well that distribution fits</p>\n\t</li>\n\t<li>\n\t<p>throw away the sample data</p>\n\t</li>\n\t<li>\n\t<p>make inferences based on the known distribution</p>\n\n\t<p>…</p>\n\t</li>\n\t<li>\n\t<p>(steal underwear)</p>\n\t</li>\n\t<li>\n\t<p>PROFIT!</p>\n\t</li>\n</ol>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Those practices continue today. It’s amazing how many [dissertations](http://lolmythesis.com/) are chock full of [p-values](http://mathworld.wolfram.com/P-Value.html), and literature surveys, and not much else. For that matter, much of the practice of [business intelligence](http://en.wikipedia.org/wiki/Business_intelligence) emerged from a similar mindset. That conceptual lens places immense pressure on data models to be “correct”, while ignoring much of the data. Caveat emptor."
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "The Two Cultures"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In 2001, a Statistics professor at UC Berkeley named Leo Breiman made waves within academic circles by publishing [Statistical Modeling: The Two Cultures](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.4933&rep=rep1&type=pdf). That paper – with a nod perhaps to [C.P. Snow](http://en.wikipedia.org/wiki/The_Two_Cultures) – chronicled a sea change. As shown by the successes of Amazon, Google, etc., the industry had begun to shift away from _data modeling_ (silos, manual process). Instead, as Breiman described it, there was increased use of _algorithmic modeling_ (machine data, automation, machine learning), which in turn led to more inter-disciplinary teams:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Check out section 7, _Algorithmic Modeling_, in the [Breiman paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.4933&rep=rep1&type=pdf). The concept is simple: first, assume that Nature is messy, complex, mysterious, glorious in its diversity, and that specific data models are not particularly robust by comparison. Draw a “black box” around the problem, and add as many factors as make sense:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The challenge then is to develop a multiplicity of different models to predict the **y**’s – evoking [Rashomon](http://en.wikipedia.org/wiki/Rashomon_effect), to paraphrase Breiman. This practice of using multiple models is called [ensembles](http://kdd13pmml.files.wordpress.com/2013/07/pattern.pdf \"\"), and we’ll return to that a bit later."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Somewhere in the late 1990s, a handful of very smart people began to automate the process of making educated guesses based on large amounts of data. Rather than isolating factors or isolating hypotheses, they leveraged algorithmic modeling, machine learning, ensembles, etc. They used the results for actionable insights in ecommerce: catalog, auction, search, ads, etc. Some entrepreneurs who led those efforts (Bezos, Omidyar, Brin, Page, et al.) became billionaires. Go figure."
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Logical Positivism Smackdown"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Putting ourselves in the shoes of someone running an ecommerce firm, let’s consider how best to create and update predictions based on algorithmic modeling."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To make this concrete, let’s say that we’re selling pre-mixed cocktails online, ordered anywhere from a tablet or laptop or smartphone, and delivered via drone to an exact GPS coordinate. Drone-based delivery comes complete with an ID check, so that those of us with gray hair tend to tip extra. We’ll call this early-stage venture _Foobartendr.io_, if we can get the domain and perhaps a term sheet too."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "One problem is that customers only scan through a few images (say, a max of 4) before they click to order. Otherwise they leave and go on to something completely different. So we need to build classifiers (Breiman’s “black box”) to predict the best 4 cocktails to show. We will leverage whatever input signals are available – those are the **x**’s that go into our model: GPS coordinate, time of day, season of the year, what’s popular recently, order history for the customer if there is any, etc."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In the context of data modeling, a standard approach would have been to pull a sample of customers who had already purchased cocktails from our service. Then we might conduct a survey among customers in the sample, to gauge their satisfaction. Then we’d come up with a model to rank the customer satisfaction per cocktail, and use those rankings to determine which cocktail images to show."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "There are several problems with that approach…"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>when new customers try out the service for the first time, we have no data about them</li>\n\t<li>when a newly created cocktail first gets created, there’s no data about it</li>\n\t<li>customers get tired of drones delivering the same old cocktail, and want to try something new</li>\n\t<li>trends change, cocktails get featured in popular media, etc., so demand is always in flux</li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "…just to name a few. To wit, the data is always in flux. Patterns of customer demand are also changing. Analysts cannot work fast enough to keep up with all those changes. Besides, there probably are many kind of subtle interactions among the inputs that simply cannot be explained in isolation. More to the point, data modeling does not work in the context of an _early-stage_ start-up which has neither customers nor data yet. How do we handle uncertainty, interactions, and updates effectively?"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "[Thomas Bayes](http://en.wikipedia.org/wiki/Thomas_Bayes) was a mathematician and minister in Britain during the 1700s, elected as a Fellow into the Royal Society in 1742. He stated a list of postulates about conditional probability in _An Essay towards solving a Problem in the Doctrine of Chances_, published posthumously in 1763. The [original](http://rstl.royalsocietypublishing.org/content/53/370) is available online; however, you may wish to read a [better transcription](http://www.stat.ucla.edu/history/essay.pdf) instead:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Bayes’ work was extended by [Pierre Simon Laplace](http://www.cs.xu.edu/math/Sources/Laplace/index.html) decades later, becoming what is now known as _Bayes Theorem_, and used for inductive reasoning. Some mathematicians and philosophers have hailed that body of work as a monumental step forward. Others have criticized it as non-science and heresy. Within the Statistics community, one encounters a big divide between _Bayesian_ and _Frequentist_ approaches. Reading through the _Comment_ and _Rejoinder_ sections of the [Breiman paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.4933&rep=rep1&type=pdf), a snapshot appears of this dispute among leading statisticians circa 2001."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Back to our problem of ordering/delivering cocktails via tablet and drone – more specifically, how do we handle interactions and updates involving large amounts of data in an uncertain and changing context? Using a Bayesian approach, we can build a large distributed framework which applies algorithmic modeling and runs efficiently in parallel at scale on a cluster, streaming through large-scale data in real-time. Using a Frequentist approach, we’re stuck with hiring famous statistics professors as expensive consultants on unending projects. We might as well roll up the business and go sell insurance, while Jeff Bezos laughs all the way to the bank. Or something."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Machine learning at scale and Bayesian approaches used together can work wonders at reversing the crippling effects of centuries of [aristotelian](http://en.wikipedia.org/wiki/Aristotelianism) perspectives encumbering the academe. James Hawthorne articulated some key issues involved, in [Giving up Judgment Empiricism: The Bayesian Epistemology of Bertrand Russell and Grover Maxwell](http://faculty-staff.ou.edu/H/James.A.Hawthorne-1/Hawthorne--Giving_Up_Judgment_Empiricism.pdf). Hawthorne quoted Grover Maxwell brilliantly in his summary:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To that point, [Karl Popper warned](http://plato.stanford.edu/entries/popper/#SciKnoHisPre) that the Scientific Method itself has a habit of skimping on accountability for its predictions. Popper criticized at length against _unconditional scientific prophecies_, based on his infamous `[C.P. + E.S.] = U.P.` formula – not unlike how poorly-wielded analytics get abused so frequently in the corporate world. We could travel down a long maze of twisty little passages about _neopositivism_. Let’s not."
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "The Diachronic Interpretation"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Instead, let’s take a good long look at a wonderfully pragmatic and useful book, [Think Bayes](http://www.greenteapress.com/thinkbayes/) by Allen B. Downey. The book is highly recommended; however, you can get a copy of its [text online](http://www.greenteapress.com/thinkbayes/thinkbayes.pdf). Scroll down to section 1.5, _The diachronic interpretation_, for an excellent overview of Bayes Theorem:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The formula leverages _conditional probability_ to infer based on what can be observed, in a way that is simple to update:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\np(H|D) = p(H) * p(D|H) / p(D)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Translated from the math, the individual components of that formula for Bayes Theorem can be described as:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "That formula summarizes the best of Bayes, Laplace, Russell, Popper, Breiman, et al., into a conceptual framework that can be coded and automated into a highly parallelized, robust, adaptive distributed framework at scale. Bingo!"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Bayesian statistics represents a credence, a degree of belief in the probability of `H` occurring given that `D` happened. More than that, it incorporates a notion of change, iteration, updates. Downey explains:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Harkening back to our friends with data models, Frequentist notions of probability focus (read: _fixate_) on outcomes, and are not particularly good at taking time and change into account. Bayesian approaches excel at incorporating change over time. They do not necessarily provide the best _predictive power_ in modeling – SVMs, Neural Nets, and Random Forests typically perform better. However, they are generally best at _transparency_: describing why particular features are being used in decisions. In particular, [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) tends to provide great transparency into Breiman’s “black box” problem. That’s important when there are questions of accountability – e.g., when investors start asking uncomfortable questions about the technology our killer app uses for suggesting cocktails."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s take a look at an example from Section 1.3, _The cookie problem_ in [Downey](http://www.greenteapress.com/thinkbayes/thinkbayes.pdf). There are two bowls o’ cookies. Some are vanilla and some are chocolate:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<table>\n<colgroup>\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n</colgroup>\n<thead><tr>\n<th style=\"text-align:left;\">container</th>\n\t\t\t<th style=\"text-align:left;\">vanilla</th>\n\t\t\t<th style=\"text-align:left;\">chocolate</th>\n\t\t</tr></thead>\n<tbody>\n<tr>\n<td style=\"text-align:left;\">Bowl 1</td>\n\t\t\t<td style=\"text-align:left;\">30</td>\n\t\t\t<td style=\"text-align:left;\">10</td>\n\t\t</tr>\n<tr>\n<td style=\"text-align:left;\">Bowl 2</td>\n\t\t\t<td style=\"text-align:left;\">20</td>\n\t\t\t<td style=\"text-align:left;\">20</td>\n\t\t</tr>\n</tbody>\n</table>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Suppose that we’re running a special on [Cookies and Cream Cocktails](http://www.yummly.com/recipes?q=cookies+and+cream+cocktail) at our _Foobartendr.io_ start-up, and we’re working on some quality assurance metrics. Given that we’ve pulled a vanilla cookie from one of the bowls, we need to calculate the probability that it came from either bowl. In other words, pulling the vanilla cookie becomes the observed data `D` , a specific bowl is a hypothesis `H` , and `p(D|H)` represents the likelihood of pulling a vanilla cookie from each of the bowls. Then we use Bayes Theorem to calculate the posterior probability `p(H|D)` for each of the bowls."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s write a simple Python script to calculate those probabilities based on Bayes Theorem. First, download the Python module [thinkbayes.py](http://www.greenteapress.com/thinkbayes/thinkbayes.py) from Downey’s web site. Then we create a new Python script named `cookie.py` in that same directory, based on using an instance of the `Pmf` class:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nfrom thinkbayes import Pmf\npmf = Pmf()\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Next, we define two hypotheses _H1_ and _H2_ – one for each bowl, respectively. The priors are `1/2` for both hypotheses, in other words a uniform distribution. We set the priors using the following calls:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\npmf.Set(\"H1\", 1 / 2.0)\npmf.Set(\"H2\", 1 / 2.0)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Next, we update with `p(D|H)` as the likelihood of pulling a vanilla cookie from each of the bowls:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\npmf.Mult(\"H1\", 30 / 40.0)\npmf.Mult(\"H2\", 20 / 40.0)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Finally, we renormalize the data and report the posterior for both hypotheses:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\npmf.Normalize()\n\nfor bowl in [\"H1\", \"H2\"]:\n    print bowl, pmf.Prob(bowl)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Saving this code to a script called `cookie.py` and running it:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\n$ python cookie.py\nH1 0.6\nH2 0.4\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The results show that the posterior probability for the Bowl 1 and Bowl 2 hypotheses are `0.6` and `0.4` respectively. Meanwhile, Downey has lots more examples shared on his blog, such as [All Your Bayes Are Belong To Us](http://allendowney.blogspot.com/2011/10/all-your-bayes-are-belong-to-us.html). Recommended reading!"
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Eight Little Numbers"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s put some Bayesian inference into practice, in the context of building a Data Science team. In a [customer experiment](http://theleanstartup.com/) at our early-stage venture _Foobartendr.io_, we tested four different UI designs for arranging the display of the top 4 cocktails. We measured how frequently customers purchased cocktails, based on each of those designs. Here are the test results:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<table>\n<colgroup>\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n</colgroup>\n<thead><tr>\n<th style=\"text-align:left;\">test ID</th>\n\t\t\t<th style=\"text-align:left;\">#visits</th>\n\t\t\t<th style=\"text-align:left;\">#purchases</th>\n\t\t</tr></thead>\n<tbody>\n<tr>\n<td style=\"text-align:left;\">#1</td>\n\t\t\t<td style=\"text-align:left;\">24</td>\n\t\t\t<td style=\"text-align:left;\">3</td>\n\t\t</tr>\n<tr>\n<td style=\"text-align:left;\">#2</td>\n\t\t\t<td style=\"text-align:left;\">180</td>\n\t\t\t<td style=\"text-align:left;\">30</td>\n\t\t</tr>\n<tr>\n<td style=\"text-align:left;\">#3</td>\n\t\t\t<td style=\"text-align:left;\">250</td>\n\t\t\t<td style=\"text-align:left;\">50</td>\n\t\t</tr>\n<tr>\n<td style=\"text-align:left;\">#4</td>\n\t\t\t<td style=\"text-align:left;\">100</td>\n\t\t\t<td style=\"text-align:left;\">15</td>\n\t\t</tr>\n</tbody>\n</table>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Let’s use this data as an interview question for Data Scientist candidates. Or, for that matter, all of the engineering and management candidates in the company. Questions:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ol>\n<li>Can you tell us which version has the lowest purchase rate?</li>\n\t<li>Why did you give that answer?</li>\n\t<li>How did you calculate it?</li>\n</ol>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This is an open-book test. Follow best practices for data analysis. Make your own choices about how to arrive at a decision with a reasonable degree of accuracy. Feel free to use a calculator, laptop, web browser, supercomputer, phone calls to Nobel laureate in-laws, etc. However, take no more than 5 minutes, max."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Data for this question was constructed so that:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>relevant calculations can be performed easily, by hand</li>\n\t<li>the question could be used for a phone screening interview</li>\n\t<li>candidates with stats background tend to take 5 minutes or less</li>\n\t<li>candidates who lack applied math skills may struggle for much longer</li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Results are easy to score, and that can be done interactively with the candidate during the interview. One reasonable solution would be to consider those eight little numbers as [point estimates](http://en.wikipedia.org/wiki/Point_estimation). In that case, an [Adjusted Wald method](http://www.measuringusability.com/wald.htm), or other similar approaches can be used to construct a _confidence interval_ around a point estimate `p` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "\nadjusted p =  (x + 2) / (n + 4)\n95% confidence interval =  ± 1.96 * sqrt(p * (1 - p) / n)\n",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Note the `2` in the numerator and the `4` in the denominator. Those represent a Bayesian practice called _shrinkage_ – which [Laplace](http://www.cs.xu.edu/math/Sources/Laplace/index.html) used infamously to describe the probability of the sun rising tomorrow. In other words, the ratio of those values `2 / 4 = 0.5` serves as the _prior_ in this case."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Try using the [Confidence Interval Calculator](http://www.measuringusability.com/wald.htm) on our data:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<table>\n<colgroup>\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n</colgroup>\n<thead><tr>\n<th style=\"text-align:left;\">test ID</th>\n\t\t\t<th style=\"text-align:left;\">#visits</th>\n\t\t\t<th style=\"text-align:left;\">#purchases</th>\n\t\t\t<th style=\"text-align:left;\">lower bounds</th>\n\t\t\t<th style=\"text-align:left;\">point estimate</th>\n\t\t\t<th style=\"text-align:left;\">upper bounds</th>\n\t\t</tr></thead>\n<tbody>\n<tr>\n<td style=\"text-align:left;\">#1</td>\n\t\t\t<td style=\"text-align:left;\">24</td>\n\t\t\t<td style=\"text-align:left;\">3</td>\n\t\t\t<td style=\"text-align:left;\">0.0351</td>\n\t\t\t<td style=\"text-align:left;\">0.1538</td>\n\t\t\t<td style=\"text-align:left;\">0.3184</td>\n\t\t</tr>\n<tr>\n<td style=\"text-align:left;\">#2</td>\n\t\t\t<td style=\"text-align:left;\">180</td>\n\t\t\t<td style=\"text-align:left;\">30</td>\n\t\t\t<td style=\"text-align:left;\">0.1189</td>\n\t\t\t<td style=\"text-align:left;\">0.1703</td>\n\t\t\t<td style=\"text-align:left;\">0.2284</td>\n\t\t</tr>\n<tr>\n<td style=\"text-align:left;\">#3</td>\n\t\t\t<td style=\"text-align:left;\">250</td>\n\t\t\t<td style=\"text-align:left;\">50</td>\n\t\t\t<td style=\"text-align:left;\">0.1549</td>\n\t\t\t<td style=\"text-align:left;\">0.2024</td>\n\t\t\t<td style=\"text-align:left;\">0.2542</td>\n\t\t</tr>\n<tr>\n<td style=\"text-align:left;\">#4</td>\n\t\t\t<td style=\"text-align:left;\">100</td>\n\t\t\t<td style=\"text-align:left;\">15</td>\n\t\t\t<td style=\"text-align:left;\">0.0919</td>\n\t\t\t<td style=\"text-align:left;\">0.1569</td>\n\t\t\t<td style=\"text-align:left;\">0.2340</td>\n\t\t</tr>\n</tbody>\n</table>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In short, the confidence intervals overlap so there’s no clear winner. We’ll need to keep running customer experiments on our cocktail promotions until we get more data."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For engineering candidates, we’ll accept the answer “No”, or “I don’t know” – with extra credit given for those who mention about calculating variance, confidence intervals, or point estimates. However, if a candidate answers “Yes” then we’ll ask why, and drill-down on the answers. If they argue the “Yes” answer, we’ll become cautious about their other interview responses."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For managment candidates, if they argue the “Yes” case, we’ll conclude the interview. Do we really want that approach to decision making on board? We might was as well hire the ghost of Aristotle."
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Corollary: Regularization"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Alternatively, we could have used a Bayesian approach to calculate [confidence limits](http://www.causascientia.org/math_stat/ProportionCI.html). That produces results comparable to the Adjusted Wald method. Frankly, the amount of [math dedicated to point estimates and intervals](http://www.stat.duke.edu/courses/Spring08/sta103/apr7/binomci.pdf) is a bit intimidating, but the actionable insights are clear. In general, most Machine Learning algorithms employ some kind of [regularization term](http://en.wikipedia.org/wiki/Bayesian_interpretation_of_regularization) to avoid _overfitting_. That’s the big idea here."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "William Chen provided an answer that cuts straight to this point on the Quora question [What are the advantages of Bayesian methods over frequentist methods in web data?](http://www.quora.com/Big-Data/What-are-the-advantages-of-Bayesian-methods-over-frequentist-methods-in-web-data)"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In that problem, Bert and Ernie are two hypothetical Quora who both have _upvotes_ and _downvotes_ on their answers. The problem is to rank users based on these votes. Using a Frequentist approach, Bert wins – even though he has less history. Those results don’t pass the smell test. To correct the results, Chen uses a `Beta(1, 1)` prior – effectively, shrinkage that adds `1` to both the numerator and denominator in the proportions data:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<table>\n<colgroup>\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n<col style=\"text-align:left;\">\n</colgroup>\n<thead><tr>\n<th style=\"text-align:left;\">user</th>\n\t\t\t<th style=\"text-align:left;\">upvote</th>\n\t\t\t<th style=\"text-align:left;\">downvote</th>\n\t\t\t<th style=\"text-align:left;\">frequentist estimate</th>\n\t\t\t<th style=\"text-align:left;\">bayesian estimate</th>\n\t\t</tr></thead>\n<tbody>\n<tr>\n<td style=\"text-align:left;\">Bert</td>\n\t\t\t<td style=\"text-align:left;\">2</td>\n\t\t\t<td style=\"text-align:left;\">0</td>\n\t\t\t<td style=\"text-align:left;\">2/(0 + 2) = 1.0</td>\n\t\t\t<td style=\"text-align:left;\">3/(3 + 1) = 0.75</td>\n\t\t</tr>\n<tr>\n<td style=\"text-align:left;\">Ernie</td>\n\t\t\t<td style=\"text-align:left;\">45</td>\n\t\t\t<td style=\"text-align:left;\">5</td>\n\t\t\t<td style=\"text-align:left;\">45/( 5 + 45) = 0.9</td>\n\t\t\t<td style=\"text-align:left;\">46/(6 + 46) = 0.88</td>\n\t\t</tr>\n</tbody>\n</table>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Ernie then ranks above Bert. Life is good."
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Key Points"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "**TODO**"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>Bayesian methods allow us to use priors to help with regularization</li>\n\t<li>streaming through large amounts of data, without having to wait for a batch window</li>\n</ul>\n"
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Suggested Books"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_Think Bayes_  \nby Allen B. Downey (2013)  \nhttp://amazon.com/dp/1449370780"
        },
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Exercises"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ol>\n<li>Write a Python script to analyze the <em>Eight Little Numbers</em> data, using the concepts presented in this chapter.</li>\n</ol>"
        }
      ],
      "metadata": {
      }
    }
  ]
}