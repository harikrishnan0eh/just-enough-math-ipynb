<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>just enough math</title>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>
    <link rel="stylesheet" type="text/css" href="theme/html/html.css"/>
  </head>
  <body data-type="book">
    <section data-type="chapter" id="idp202704" data-pdf-bookmark="Chapter 5. Caterpillar Won’t Be Building A Social Network">
<h1>Caterpillar Won’t Be Building A Social Network</h1>

<p><strong>Topics:</strong> <em>Optimization</em>, <em>Linear Programming</em>, <em>Operations Research</em>, <em>Stochastic Gradient Descent</em>, <em>Evolutionary Algorithms</em>, <em>Genetic Programming</em></p>

<blockquote> </blockquote>

<hr/>
<p>The top ten stocks listed in the <a href="https://www.google.com/finance?catid=us-TRBC%3A5210202010&amp;sort=MARKET_CAP&amp;ei=js_KUpD9GsWUiAKCmAE">Heavy Machinery and Vehicles</a> sector have a combined market capitalization of approximately $150B, circa early 2014. <a href="http://www.cat.com/">Caterpillar</a>, an iconic firm within that sector, accounts for more than one third of that figure. Mere mention of Caterpillar conjures images of towering, noisome behemoths of steel, diesel, and thunder – the utter bane of the <a data-original-title="" href="http://www.amazon.com/dp/0061129763" title="">The Monkey Wrench Gang</a>, if you will. Rugged souls, salt of the Earth types, toiling long hours in dangerous environments, endeavoring at the apogee of industrial culture in its urgent quest to master Nature. Not quite the image of pierced and tattooed <a href="http://www.linkedin.com/today/post/article/20140105233439-24171-techbrats-goldberg-shih-and-gopman-do-not-represent-technology?trk=eml-ced-b-art-M-0&amp;ut=0nTh2SkSxHfS41">hipsters</a> just back from <a data-original-title="" href="http://www.burningman.com/" title="">Burning Man</a>, zooming deftly on their <a href="http://bit.ly/1cTkLnY">fixie bikes</a> as they sip light-roast latte in smug nonchalance, speeding through red lights across the chicest paths in San Francisco, late for some crucial meeting/wine tasting at a tech start-up… some new, hot social network venture bringing a highly gamified sharing economy to iguana owners, like an Uber for herpetophiles. Which, <a href="http://valleywag.gawker.com/">their VCs</a> say, will be huge – probably several billion in valuation at IPO. Or something.</p>

<p>When Cat begins to invest in machine learning, cluster computing, algorithmic modeling, etc., at levels of R&amp;D spend comparable with, say, Facebook or Twitter, they won’t be building a social network. They will be optimizing real business – business worth billions in durable goods and industrial plant. Not the Silicon Valley notion of billions that evaporates overnight. Optimizing for supply chain, engine designs, production schedules, warehouse stocking, pricing strategies, delivery routes, etc. You can bet serious coin on that.</p>

<p>The heavy equipment manufacturing sector will also be treads-deep, so to speak, in data. Data from “Internet of Things” <a href="http://www.ni.com/white-paper/14667/en/">sensor arrays</a>, <a href="http://www.planet-labs.com/">microsats</a>, <a href="http://titanaerospace.com/platforms/solara-50/">atmostats</a>, etc., looms on the horizon for industrial firms such as Caterpillar. Data which must be confronted and leveraged, before a competitor seizes first-mover advantage. The skills and tooling which served so well to earn billions in ad-tech are not necessarily the same skills and tooling needed in the business optimization challenges ahead.</p>

<h2>Linear Programming</h2>

<p>Recall from the previous chapter the case of more than <em>N</em> equations with <em>N</em> variables. We call that an <a href="http://en.wikipedia.org/wiki/Overdetermined_system">overdetermined system</a>. That, as we saw, presents a case in which there is no solution. So we approximated a solution. In contrast, let’s now consider the case we have less than <em>N</em> equations with <em>N</em> variables. We call that an <a href="http://en.wikipedia.org/wiki/Underdetermined_system">underdetermined system</a> A key issue in that case is that there are many (if not <em>infinitely</em> many) possible solutions. So we must optimize to choose among the best ones.</p>

<p>One of the popular approaches to optimization is called <a href="http://en.wikipedia.org/wiki/Linear_programming">linear programming</a>. Not so much in the sense of “computer programming”, but rather <a href="http://en.wikipedia.org/wiki/Mathematical_optimization">mathematical programming</a> that involves linear systems. We’ll stick with the equation  <span class="math-tex" data-type="tex">\(\mathbf{A}\mathbf{x} = \mathbf{y}\)</span> for the general form of matrix representation here, but the algorithms become somewhat different.</p>

<p>Linear programming – as an approach for solving a system of linear inequalities – dates back to the 18th century. However, it came to the fore as an important discipline during World War II. Logistics was the endeavor that won or lost battles waged far from the heartland. Mathematical optimization techniques made it possible for countries to adjust their logistics – and estimate those of their enemies – during crucial times.</p>

<p><a data-original-title="" href="http://en.wikipedia.org/wiki/George_Dantzig" title="">George Danztig</a> did not invent linear programming, but he is credited with both the first published paper and some of the most significant contributions to the field through the years. Danztig is also the source of a popular urban legend about a grad student who walks in late to a math class, then quickly writes down two problems listed on the board – thinking they were assigned homework, and not realizing that the professor had started the lecture by listing unresolved problems in mathematics. Danztig did that quite famously in <a href="http://rev-inv-ope.univ-paris1.fr/files/26305/IO-26305-1.pdf">Jerzy Neyman’s class at UC Berkeley</a>:</p>

<blockquote>
<p><em>A few days later I apologized to Neyman for taking so long to do the homework — the problems seemed to be a little harder than usual. I asked him if he still wanted it. He told me to throw it on his desk. I did so reluctantly because his desk was covered with such a heap of papers that I feared my homework would be lost there forever. About six weeks later, one Sunday morning about eight o’clock, [my wife] Anne and I were awakened by someone banging on our front door. It was Neyman. He rushed in with papers in hand, all excited: “I’ve just written an introduction to one of your papers. Read it so I can send it out right away for publication.”</em></p>
</blockquote>

<p>In other words, we’re reviewing some raw genius here. The genius in linear programming was to apply linear algebra and optimization theory to cut the time required by complicated logistics problems, in some cases by orders of magnitude. What’s more, the <a href="https://www.cs.duke.edu/courses/spring07/cps296.2/papers/LinearProgramming_article.pdf">Simplex Method</a> of linear programming dovetailed with the emerging field of Computer Science, so this kind of work could be automated and to some extent parallelized. The military jumped on this opportunity, followed closely by industry. AT&amp;T Bell Labs was one of the largest use cases for decades.</p>

<p>Suppose we have an <em>objective function</em> stated as the product of two vectors <em>z</em>(<strong>x</strong>) = <strong>cx</strong>, and a <em>feasible region</em> <strong>Ax</strong> = <strong>b</strong> where <em>x</em><sub>i</sub> ≥ 0 holds. The former is what we need to optimize – minimize or maximize. The latter represents a set of constraints. Simple(x) enough.</p>

<p>As an example, recall our business venture from previous chapters: an automated cocktail delivery service, ordered by tablet and delivered by drone. Based on extensive analysis of large-scale data, we have found that three categories of potations represent nearly all of our company net revenue:</p>

<ul>
	<li><a href="http://www.amazon.com/dp/0965433323">Pre-Prohibition</a></li>
	<li><a data-original-title="" href="http://en.wikipedia.org/wiki/Liquid_nitrogen_cocktail" title="">Liquid Nitrogen</a></li>
	<li><a href="http://www.ahamodernliving.com/blog/garden/garden-cocktails/">Freshly Muddled</a></li>
</ul>

<p>Our sales have booked a 5:3:2 ratio for net revenues from these three categories, and our investors mentioned that we need to optimize revenue this next quarter. Or something bad might happen. So let’s define an objective function <em>z</em>(<strong>x</strong>) based on <em>x</em><sub>i</sub> representing those three categories:</p>

<p><em>z</em>(<strong>x</strong>) = 5<em>x</em><sub>1</sub> + 3<em>x</em><sub>2</sub> + 2<em>x</em><sub>3</sub></p>

<p>This revenue optimization is subject to a set of constraints, namely our costs:</p>

<ul>
	<li>ingredients</li>
	<li>bartender salaries</li>
	<li>drone delivery</li>
</ul>

<p>As it turns out, all of the ingredients last quarter cost us $100K, and were distributed among the three cocktail categories in a 1:1:3 ratio. Our bartenders work fast – largely due to our in-house massage therapy, free gourmet meals, and complementary fixie bike repair service – all lumped into their loaded salaries. However, they still take their own sweet time when it comes to mixing perfect drinks. Salaries for bartenders totaled $600K last quarter, and the overall time per cocktail category clocked in at a 10:4:5 ratio. Bar charts don’t lie. Meanwhile, drone-based delivery is still somewhat of an art form. We lost our shirts, shelling out a whopping $150K last quarter just for deliveries. Turns out that muddled drinks tend to be not only trendy but much heavier, so our airborne delivery costs for the different categories were in a 1:1:3 ratio. Oddly enough, we were fortunate to barter for our remaining expenses, such as accounting, datacenter, marketing, etc. Most of those services were being run by um, “social” drinkers.</p>

<p>We can represent these cost constraints by a system of linear inequalities:</p>

<p><em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> + 3<em>x</em><sub>3</sub> ≤ 100<br/>
10<em>x</em><sub>1</sub> + 4<em>x</em><sub>2</sub> + 5<em>x</em><sub>3</sub> ≤ 600<br/>
<em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> + 3<em>x</em><sub>3</sub> ≤ 150</p>

<p>Plus, there are the additional constraints such that all variables are non-negative. In other words, we’re not going to give people money to take our drinks:</p>

<p><em>x</em><sub>1</sub> ≥ 0, <em>x</em><sub>2</sub> ≥ 0, <em>x</em><sub>3</sub> ≥ 0</p>

<p>NB: these constraints are not equations. We turn them into equations by introducing <a href="http://en.wikipedia.org/wiki/Slack_variable">slack variables</a>. The theory behind this is probably overkill for the moment, but the point is that introducing these variables makes the system underdetermined. That’s a good thing, for optimization.</p>

<p>We can run this linear programming example in Python using the <a href="https://pypi.python.org/pypi/glpk/0.3">PyGLPK</a> package. That’s a Python wrapper for the <a href="http://www.gnu.org/software/glpk/">GLPK</a> solver, which is based on a subset of the <a href="http://www.ampl.com/BOOK/index.html">AMPL</a> mathematical programming language. Let’s create a Python script <code>lp_example.py</code> to model our cocktail net revenues:</p>

<pre data-original-title="" title="">
<code>from glpk import LPX

# set up to maximize the objective function
lp = LPX()
lp.name = 'example'
lp.obj.maximize = True

# append 3 rows, named p, q, r
row_names = ["p", "q", "r"]
lp.rows.add(len(row_names))

for r in lp.rows:
    r.name = row_names[r.index]

lp.rows[0].bounds = None, 100.0
lp.rows[1].bounds = None, 600.0
lp.rows[2].bounds = None, 150.0

# append 3 cols, named x0, x1, x2
lp.cols.add(3)

for c in lp.cols:
    c.name = 'x%d' % c.index
    c.bounds = 0.0, None

# set the objective coefficients and
# non-zero entries of the constraint matrix
lp.obj[:] = [ 5.0, 3.0, 2.0 ]
lp.matrix = [ 1.0, 1.0, 3.0, 10.0, 4.0, 5.0, 1.0, 1.0, 3.0 ]

# report the objective function value and structural variables
lp.simplex(msg_lev=LPX.MSG_ALL)
print 'Z = %g;' % lp.obj.value,
print '; '.join('%s = %g' % (c.name, c.primal) for c in lp.cols)
</code></pre>

<p>Running this script, we solve for the values in <strong>x</strong> that maximize the objective function <em>z</em>(<strong>x</strong>):</p>

<pre data-original-title="" title="">
<code>$ python src/lp_example.py
*     0: obj =   0.000000000e+00  infeas =  0.000e+00 (0)
*     2: obj =   3.666666667e+02  infeas =  0.000e+00 (0)
OPTIMAL SOLUTION FOUND
Z = 366.667; x0 = 33.3333; x1 = 66.6667; x2 = 0
</code></pre>

<p>In other words, to maximize net revenues next quarter and keep the investors off our backs, we need to ditch the Freshly Muddled drinks. Then set our sales targets at 1/3 Pre-Prohibition and 2/3 Liquid Nitrogen. Done and done. Then we will anticipate (read: leak via press release) an embarrassingly ginormous $367K net revenues on approximately $850K costs, which gives us a 30% operating margin. Of course, since this is Silicon Valley, given that projection we are compelled to “embellish” and forecast a 62% operating margin while backdating our stock options. Buy low, sell high.</p>

<h2>Alternating Directions</h2>

<p>Optimization is not limited to linear programming. Dantzig himself received immediate criticism against Simplex Method, with critics pointing out that the real world is non-linear. And so a wide body of work in <a href="http://en.wikipedia.org/wiki/Operations_research">Operations Research</a> has emerged, seeking to optimize different kinds of computationally difficult problems.</p>

<p>One really great resource is the <a href="http://www.stanford.edu/group/SOL/">Systems Optimization Laboratory</a> (SOL) at Stanford, which Dantzig helped establish in the 1973. In particular, check out their <a href="http://www.stanford.edu/group/SOL/download.html">software</a> links for various solvers. These get used throughout industry – ranging from industrial plant optimization to macroeconomic policy to aircraft design, and much more. An excellent text from SOL is <a href="http://www.stanford.edu/group/SOL/Books/0976401304.pdf">Convex Optimization &amp; Euclidean Distance Geometry</a> by Jon Dattorro. A reader will likely recognize many of the linear algebra concepts covered in the previous chapter. The <a href="http://www.ampl.com/BOOK/index.html">AMPL book</a> by Fourer, Gay, Kernighan (listed at the end of this chapter) is also an excellent reference for how to approach these kinds of optimization problems.</p>

<p>Another research group at Stanford, the <a href="http://isl.stanford.edu/">Information Systems Laboratory</a> (ISL) in the EE Department, has published exceptionally interesting work in convex optimization. The paper <a href="http://www.stanford.edu/~boyd/papers/admm_distr_stats.html">Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</a> (ADMM) by Stephen Boyd, Neal Parikh, et al. Their approach is summarized in the introduction:</p>

<blockquote>
<p>Many such problems can be posed in the framework of convex optimization. Given the significant work on decomposition methods and decentralized algorithms in the optimization community, it is natural to look to parallel optimization algorithms as a mechanism for solving large-scale statistical tasks. This approach also has the benefit that one algorithm could be flexible enough to solve many problems.</p>
</blockquote>

<p>In other words, even though there are many different algorithms used in machine learning, the bulk of these can be defined by a common mathematical formulation. Much of machine learning becomes essentially an optimization problem. This is described later on page 38:</p>

<blockquote>
<p>ADMM explicitly targets problems that split into two distinct parts, <em>f</em> and <em>g</em>, that can then be handled separately. Problems of this form are pervasive in machine learning, because a significant number of learning problems involve minimizing a loss function together with a regularization term or side constraints.</p>
</blockquote>

<p>Two key components are identified: a <em>loss function</em> and a <em>regularization term</em>. The former recalls what we did in the linear algebra examples with approximation. The latter recalls what we did in the bayesian statistics examples by leveraging priors.</p>

<p>Moreover, the techniques used in ADMM can run in parallel on large-scale data, based on open source frameworks. For example, analysis comparing ADMM implementations on Hadoop vs. Spark is given in <a href="http://www3.decf.berkeley.edu/~jreilly/#cachereduce">Cache-Reduce: Implementing Collaborative Filtering on a Cluster with the Spark Framework</a> by Johann Grauzam, Boris Prodhomme, and Jack Reilly:</p>

<blockquote>
<p>This algorithm lends itself particularly well to the map-reduce framework for parallel computing, except for one particular downside. Map-reduce is typically involves a single map-reduce process, which means that state need not be persistent over map jobs, since they are one-off jobs. In ADMM, however, this is not the case as this is an iterative algorithm where the <em>A</em><sub>i</sub>, <em>b</em><sub>i</sub> data are needed for each iteration of <em>x</em><sub>i</sub>, <em>u</em><sub>i</sub>. Loading the data for each successive iteration of the algorithm can be expensive, in particular in distributed systems, where network communication is the bottleneck.</p>
</blockquote>

<p>If the premise from Boyd, et al., is correct that much of the current work in machine learning software development can be reduced to an optimization of two functions <em>f</em> and <em>g</em>, and the analysis from Reilly, et al., is correct that this highly-valued optimization does not fit MapReduce particularly well, then it’s probably time for those of us holding Hadoop-related stock options to begin flipping them. Now or never.</p>

<h2>Gradient Descent</h2>

<p>If we were to pull a <a data-original-title="" href="https://code.google.com/p/googleclusterdata/" title="">cluster trace</a> from production servers running right now in any randomly selected datacenter, those resources likely get spent on the following four kinds of work:</p>

<ul>
	<li><a href="http://stackoverflow.com/questions/12717659/hadoop-reduce-shuffle-merge-in-memory">moving data between different servers</a><br/>
	(what others think your Data Scientists do)</li>
	<li><a href="http://www.amazon.com/dp/B008HMN5BE">cleaning up data prior to use, manually</a><br/>
	(what your Data Scientists really do)</li>
	<li><a href="http://www.umiacs.umd.edu/~jimmylin/publications/Lin_BigData2013.pdf">something akin to stochastic gradient descent</a><br/>
	(what your Data Scientists think they do)</li>
	<li><a href="http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf">highly-available request/response services</a><br/>
	(what your Data Scientists should think about doing)</li>
</ul>

<p>ADMM uses an optimization framework called <em>gradient ascent</em> to find the maximum for an objective function. The flip side of that same optimization framework is called <a href="http://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>, finding the minimum for an objective function. Both came out of work by <a href="http://en.wikipedia.org/wiki/Laplace's_method">Laplace, et al.</a> as a way of approximating integrals. At this point, an cornucopia of great use cases exist.</p>

<p>After we’ve cleaned up our data, formulated workflows in terms of monoids, done our graph representation, and parallelized it all with a wealth of linear algebra, the heavy-lifting that remains for the computers to perform is probably something closely akin to gradient descent. For example, Jeff Dean describes a large use case for the many layers of neural nets used in <a href="https://plus.google.com/+ResearchatGoogle/posts/C1dPhQhcDRv">deep learning</a> at Google. We’re probably all going to be using some variant gradient descent for a long while.</p>

<p>Let’s take a look at how that works – and here’s where we actually get to apply some Calculus! Suppose that in our tablet/drone cocktail delivery start-up, based on extensive analysis of large-scale data, we have found that the cost of losing <a data-original-title="" href="http://www.amazon.com/dp/B000H2XXAA" title="">cocktail parasol drink umbrellas</a> turns out to be a function of the height at which the drones fly. Perhaps we used least squares approximation to handle that curve fitting. Or something. Anywho, <em>f</em>(<em>x</em>) = <em>x</em><sup>4</sup> - 3 <em>x</em><sup>3</sup> + 2 represents the umbrella loss <em>f</em>(<em>x</em>) in terms of the drones’ mean flying height <em>x</em>, so that defines the objective function we need to minimize.</p>

<p><strong>Step 1</strong>: find the first derivative of the objective function. Frankly, we could use calculus at that point to determine the solution for a simple function like this. But let’s pretend the problem is something harder, much much harder. <strong>Step 2</strong>: write a Python script <code>gd.py</code> that implements gradient descent:</p>

<pre data-code-language="" data-original-title="" title="">
<code>
## based on an analytic solution, we expect that the local minimum
## of the polynomial function f(x) occurs at x = 9/4 = 2.25

def f (x):
    return x**4 - 3 * x**3 + 2

def f_prime (x):
    """first derivative of f(x)"""
    return 4 * x**3 - 9 * x**2

def gradient_descent (x1, toler, step_size, max_iter, func, func_deriv):
    """inputs: f, starting value x1, termination tolerances"""
    x0 = 0
    print "\t".join([ "i", "x1", "error", "func(x1)" ])

    for i in xrange(max_iter):
        error = abs(x1 - x0)
        report = ["%d %0.4e %0.4e %3.4e" % (i, x1, error, func(x1))]
        print "\t".join(report)

        if abs(x1 - x0) &lt;= toler:
            return "local minimum:", x1

        x0 = x1
        x1 = x0 - step_size * func_deriv(x0)

    return "halted"

## NB: step size is set here, it is not adaptive

x1 = 6
toler = 0.00001
step_size = 0.01
max_iter = 93

print gradient_descent(x1, toler, step_size, max_iter, f, f_prime)
</code></pre>

<p>Bokay. That was not difficult. One big loop, really. <strong>Step 3</strong>: run that <code>gd.py</code> script to obtain the answer:</p>

<pre data-original-title="" title="">
<code>$ python src/gd.py 
i   x1          error       func(x1)
0   6.0000e+00  6.0000e+00  6.5000e+02
1   6.0000e-01  5.4000e+00  1.4816e+00
2   6.2376e-01  2.3760e-02  1.4233e+00
3   6.4907e-01  2.5309e-02  1.3571e+00
4   6.7605e-01  2.6978e-02  1.2819e+00
5   7.0482e-01  2.8774e-02  1.1964e+00
6   7.3553e-01  3.0704e-02  1.0989e+00
7   7.6830e-01  3.2773e-02  9.8789e-01
8   8.0328e-01  3.4985e-02  8.6137e-01
9   8.4062e-01  3.7341e-02  7.1727e-01
10  8.8046e-01  3.9837e-02  5.5332e-01
11  9.2293e-01  4.2467e-02  3.6711e-01
...
66  2.2499e+00  2.2213e-05  -6.5430e+00
67  2.2499e+00  1.7716e-05  -6.5430e+00
68  2.2499e+00  1.4129e-05  -6.5430e+00
69  2.2500e+00  1.1268e-05  -6.5430e+00
70  2.2500e+00  8.9864e-06  -6.5430e+00
('local minimum:', 2.2499646074278457)
</code></pre>

<p>Clearly, we must attempt to fly our drones at a height of 2.25 meters, to minimize the loss of drink umbrellas. Hopefully we won’t run into any Olympic basketball players. Hopefully.</p>

<blockquote> </blockquote>

<p>Gradient descent is a <em>deterministic</em> algorithm, in other words you get the same results every time you run GD with the same data, at the same cost. In practice, problems become much more complex than what’s shown above. In lieu of having a well-defined, differentiable function <em>f</em>(<em>x</em>), we’re probably creating some classifier based on a training set of data, working with lots of approximations, error terms, etc. In other words, building a <em>learner</em>. While it may involve some mathematical tricks and transforms to arrive at a differentiable objective function, the basic idea is to tune parameters in the learner such that it:</p>

<ol>
	<li>minimizes error when the learner’s predictions are compared with the training set</li>
	<li>penalizes solutions that veer toward overfitting (i.e., regularization)</li>
</ol>

<p>With GD, we must run through the entire training set on each iteration. That’s expensive. Instead we generally use a faster variant called <a href="http://scikit-learn.org/stable/modules/sgd.html">stochastic gradient descent</a> (SGD). Being a <em>stochastic</em> algorithm, it hops and skips its way gracefully through the training set, “learning” from each new data point encountered. SGD is more efficient, but it introduces more knobs and dials to adjust. It also becomes sensitive to how the data is prepared, in terms of <a href="http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling">feature scaling</a>. There are trade-offs in life.</p>

<p>For a great deep-dive on SGD, check out this excellent <a href="http://www.iro.umontreal.ca/~pift6266/H10/notes/gettingstarted.html#stochastic-gradient-descent">Python tutorial</a> by Yoshua Bengio @Université de Montréal. Also check out the <a href="http://scikit-learn.org/stable/modules/sgd.html">scikit-learn</a> examples. Good stuff.</p>

<blockquote> </blockquote>

<p>Looking a few more years ahead, one of the promises of <a href="http://www.dwavesys.com/en/products-services.html">quantum computing</a> is to be able to knock down huge <a data-original-title="" href="http://arxiv.org/abs/quant-ph/0405146" title="">gradient descent problems in constant time</a>. Think about that: if we’ve reworked our high-ROI apps to leverage lots of machine learning, algorithmic modeling, abstract algebra, functional programming, bayesian stats, graph theory, sparse matrices, parallelism, cluster computing, etc., while the heavy-lifting on those clusters really comes down to performing lots of gradient descent, then SGD represents much of our datacenter cost. How about slashing that cost by orders of magnitude? Just plug in quantum algorithms… it recalls the oft-told line by grad students during thesis defense, “That was made pluggable.”</p>

<p><a data-original-title="" href="http://youtu.be/nD-iVXglyTQ" target="_blank" title=""><img alt="qCraft video on YouTube" src="http://img.youtube.com/vi/nD-iVXglyTQ/0.jpg"/></a></p>

<p>How likely is that scenario, or rather how far away could we anticipate it in production? It’s still a matter of research, true. However, one might take a hint from a recent twist… The <a href="https://plus.google.com/u/1/+QuantumAILab/posts/grMbaaDGChH">Google Quantum AI team</a> released a Minecraft mod called <a href="http://qcraft.org/">qCraft</a>, with the intent of identifying non-linear thinkers who are adept at manipulating quantum problems. Identifying them early. Those of you who have kiddos probably understand that 10-year-olds play Minecraft. Let’s do the math … some subset of these avid Minecraft players will become Google AI interns in less than a decade. Meanwhile, the fine people at Google have a track record to uphold for leveraging huge technology disruptions.</p>

<h2>Evolutionary Algorithms</h2>

<p>Meanwhile, for those of us who don’t own a <a href="http://www.dwavesys.com/en/products-services.html">DWave</a> quantum computer, sometimes the SGD approach won’t work. Perhaps the parameter space is overly large… We showed a simple example of using PCA for dimensional reduction, but in general it’s a hard problem. Perhaps we cannot get a derivative of the objective function… Or, perhaps even specifying an objective function is a problem? Reading from Dantzig and others, enormous bias can be introduced through subtle nuances in objective functions. Perhaps the best that can be done is to propose a set of “reasonably good” candidate solutions – prompting experts and computers to work together. Stranger things have happened.</p>

<p>In that circumstance, another approach is to leverage <a href="http://en.wikipedia.org/wiki/Evolutionary_algorithm">Evolutionary Algorithms</a> – in other words, using a search heuristic that mimics the process of natural selection in biological evolution. The idea is to generate candidate solutions which may or may not be used directly, but help inform domain experts how to derive novel designs from first principles. For example, think of it as a way to accelerate design iterations.</p>

<p><a href="http://www.cs.oswego.edu/~blue/hx/courses/cogsci1/s2001/section05/subsection5/main.html">John Holland</a> is credited with many advances in this area. To give some historical context, check out this report – <a href="http://deepblue.lib.umich.edu/bitstream/handle/2027.42/5578/bac4296.0001.001.pdf">Machine Adaptive Systems</a> from 1963 – where Holland proposed using evolutionary algorithms to build a kind of machine learning classifier called <a href="http://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural networks</a>.</p>

<p>Let’s focus on an important subclass is called <a href="http://en.wikipedia.org/wiki/Genetic_programming">genetic programming</a> (GP), used to synthesize computer programs that perform some user-defined task. A good reference in general for GP is <a href="http://dces.essex.ac.uk/staff/rpoli/gp-field-guide/A_Field_Guide_to_Genetic_Programming.pdf">A Field Guide to Genetic Programming</a> by Riccardo Poli, William Langdon, Nicholas McPhee. Also see the text <em>Genetic Programming in Theory and Practice</em><br/>
edited by Rick Riolo, Bill Worzel, et al., which is listed at the end of this chapter.</p>

<p>On the one hand, GPs solve interesting commercial problems and don’t require a whole lot of code. On the other hand, the problem space fits well for distributed computing at scale. In Python, a great open source framework for GPs is <a href="https://github.com/perone/Pyevolve">Pyevolve</a>. Another distributed framework for running GPs at scale is <a href="https://github.com/ceteri/exelixi/wiki">Exelixi</a>. The latter scales-out based on <a href="http://mesos.apache.org/">Mesos</a> clusters. Check out the sample code, which includes <a href="https://github.com/ceteri/exelixi/wiki/Zen-and-the-Art-of-Bicycle-Route-Planning">route scheduling</a> (young Steve Jobs doing weekend chores on his bike), and <a href="https://github.com/ceteri/exelixi/wiki/HSA-Lawnmower-Drones">program generation</a> (programming drones, which seems oddly familiar).</p>

<blockquote> </blockquote>

<p>Imagine if we could apply GP technology to <em>fix</em> software? That’d be great. Imagine data mining at scale using the public archives of <a href="https://github.com/">GitHub</a> and other public code repositories. Imagine applying a myriad of machine learning techniques to generalize about bug fixes by examining the commit histories for thousands of popular open source software projects. Turn the deltas into “operations” and evolve ways to apply common patches to other OSS projects. Then perhaps, just maybe, we could apply GPs to evolve bug fixes. Oh snap! A researcher named <a href="http://www.cs.cmu.edu/~clegoues/">Claire Le Goues</a> @CMU has been busy already doing that. Check out the paper <a href="http://www.cs.virginia.edu/~weimer/p/weimer-icse2012-genprog-preprint.pdf">A Systematic Study of Automated Program Repair: Fixing 55 out of 105 Bugs for $8 Each</a> by Le Goues, et al. Amazingly brilliant.</p>

<blockquote> </blockquote>

<hr/>
<h2>Key Points</h2>

<ul>
	<li>operations research – e.g., techniques for optimization… in other words, ROI</li>
	<li>Caterpillar won’t be building a social network, they’ll be optimizing supply chain</li>
	<li>ad-tech in ecommerce didn’t need much, but IoT requires huge amounts</li>
	<li>stochastic gradient descent and related techniques: heavy lifting</li>
	<li>quantum algorithms could leverage this area into an enormous game-changer</li>
	<li>evolutionary algorithms for handling more complex problems</li>
</ul>

<h2>Suggested Books</h2>

<p><em>Linear Programming and Extensions</em><br/>
by George Danzig (1963)<br/>
<a href="http://amazon.com/dp/0691059136">http://amazon.com/dp/0691059136</a></p>

<p><em>Convex Optimization &amp; Euclidean Distance Geometry</em><br/>
by Jon Dattorro (2006)<br/>
<a data-original-title="" href="http://amazon.com/dp/0615193684" title="">http://amazon.com/dp/0615193684</a></p>

<p><em>AMPL: A Modeling Language for Mathematical Programming</em><br/>
by Robert Fourer, David M. Gay, Brian Kernighan (1997)<br/>
<a data-original-title="" href="http://amazon.com/dp/0534388094" title="">http://amazon.com/dp/0534388094</a></p>

<p><em>A Field Guide to Genetic Programming</em><br/>
by Riccardo Poli, William Langdon, Nicholas McPhee (2008)<br/>
<a data-original-title="" href="http://amazon.com/dp/1409200736" title="">http://amazon.com/dp/1409200736</a></p>

<p><em>Genetic Programming in Theory and Practice</em><br/>
edited by Rick Riolo, Bill Worzel, et al. (2003)<br/>
<a href="http://amazon.com/dp/1402075812">http://amazon.com/dp/1402075812</a></p>

<h2>Exercises</h2>

<p><b>TODO</b></p>
</section>
  </body>
</html>
