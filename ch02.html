<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>just enough math</title>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>
    <link rel="stylesheet" type="text/css" href="theme/html/html.css"/>
  </head>
  <body data-type="book">
    <section data-type="chapter" id="idp10304" data-pdf-bookmark="Chapter 2. Show Me The Monoid">
<h1>Show Me The Monoid</h1>

<p><strong>Topics:</strong> <em>Abstract Algebra</em>, <em>Category Theory</em>, <em>Functional Programming</em>, <em>Data Workflows</em>, <em>Parallel Processing</em>, <em>Combinatorics</em></p>

<hr/>
<p>The 1996 film <a data-original-title="" href="http://en.wikipedia.org/wiki/Jerry_Maguire" title="">Jerry Maguire</a> featured a famous line, “Show me the money!” Many advanced machine learning practices that run on Big Data leverage a variant of the Hollywood line from that movie: “Show me the monoid!” Why? Because there’s lots of number-crunching to perform, and parallel processing is generally a good way to avoid having your number-crunching job take days/weeks/months to run. However, parallel processing is hard and the “popular” techniques have tended to be obscure and require sophisticated programming and even more sophisticated and expensive operations. Fortunately a new class of open source technology -- led by Twitter<a contenteditable="false" data-original-title="" data-primary="Twitter" data-type="indexterm" title="" id="idm25040"> </a>, eBay, LinkedIn, and some other firms -- has made parallel processing much simpler and less expensive. You’ll need to understand a bit of abstract algebra to make sense of the functional programming that gets used. That in turn will help you build high-ROI apps at scale.</p>

<p><a data-original-title="" href="http://en.wikipedia.org/wiki/Jerry_Maguire" style="line-height: 1.5em;" title=""><img alt="Show Me The Monoid" src="http://delimiter.com.au/wp-content/uploads/2011/03/jerrymaguire.jpg"/></a></p>

<p>For example, if you are working with MapReduce<a contenteditable="false" data-primary="MapReduce" data-type="indexterm" id="idm120960"> </a> programs – that is, if you are working with efficient MapReduce programs at scale, running as high-ROI apps – then your code probably leverages a couple of important notions implicitly (1) a <em>monoid<a contenteditable="false" data-primary="monoid" data-type="indexterm" id="idp6944"> </a></em> pattern, and (2) a <em>barrier</em> pattern. Your code may not acknowledge that fact, and quite frankly the engineers on your team may not even know about these or be able to describe them. However, MapReduce (e.g., <a data-original-title="" href="http://hadoop.apache.org/" title="">Apache Hadoop</a>) is a simplified combination of those two patterns -- then implemented as a fault-tolerant batch scheduler atop clusters of commodity hardware.</p>

<p>By understanding the design patterns and the underlying math we can explore beyond Hadoop, and begin to look at more advanced approaches to machine learning at scale for production use.</p>

<blockquote> </blockquote>

<p>The notion of <em>monads</em> traces back in Greek philosophy (or earlier) to Pythagoras. That was revisited in great detail by Renaissance mathematicians such as <a data-original-title="" data-primary="John Dee" data-secondary="" data-tertiary="" href="http://www.esotericarchives.com/dee/monad.htm" title="">John Dee</a>, Giordano Bruno, Gottfried Leibniz, et al. More contemporary usage of the term <em>monoid</em> was introduced by <a data-original-title="" data-primary="Claude Chevalley" data-secondary="" data-tertiary="" href="http://books.google.com/books?id=vv7zkx_cSqYC&amp;printsec=frontcover&amp;source=gbs_atb#v=onepage&amp;q&amp;f=false" title="">Claude Chevalley</a> in 1956 as “an internal law of composition”, a notion which subsequently migrated into computer science. The big idea here is that we want to be able to compute functions using really large-scale data. The kinds of functions we want to compute follow the old, familiar form of:</p>

<pre data-original-title="" title="">
<code>y = f(x)
</code></pre>

<p>Simple enough. Based on the functions we define, we need to build and run Enterprise data workflows – employing highly robust approaches, not just twiddling bits here. Solid engineering. Envision requirements that involve lots of data sources, lots of use cases, lots of functions to compute. The math looks more like:</p>

<pre data-original-title="" title="">
<code>y = f(x)
v = g(y)
u = h(v)
</code></pre>

<p>… And so on. Since we have some complicated data workflows<a contenteditable="false" data-primary="data workflows" data-type="indexterm" id="idp20576"> </a> to define, those functions end up being stated as <em>dependency graphs</em>, where one computed data product flows into another. Consequently, we’ll see lots of code in which the functions get <em>composed<a contenteditable="false" data-primary="function composition" data-type="indexterm" id="idm60512"> </a></em>, along the lines of:</p>

<pre data-original-title="" title="">
<code>u = h(g(f(x)))
</code></pre>

<p>Great. That should be simple to compute, right? However – and it becomes quite a big honking <strong>HOWEVER</strong> in high-ROI apps at scale – this is not particularly simple to compute when the data grows large and the functions become complex.</p>

<p>Let’s consider a use case. Imagine that you run Twitter<a contenteditable="false" data-primary="Twitter" data-type="indexterm" id="idm77072"> </a>. You need to compute followers to recommend, where to place sponsored tweets, etc. You have hundreds of millions of users, each of which probably has maybe a few hundred followers. So you could, for example, write code on your laptop to analyze the tweets for each user, leverage <a href="http://pyevolve.sourceforge.net/wordpress/?p=2497">Cosine Similarity</a> to compare interests between users, then make recommendations among their followers…</p>

<p>Ka’chunk, ka’chunk, your laptop computer crunches through each user’s data. Operations get repeated billions of times on different data. That’ll take a while to process. Moreover, some users – Taylor Swift, Lady Gaga, Barak Obama, Katy Perry, et al., – have <em>millions</em> of followers. Your laptop keeps running out of memory when processing recommendations for those. Or worse, it crashes in the midst of one of the “heavy hitters<a contenteditable="false" data-primary="heavy hitters" data-type="indexterm" id="idm112608"> </a>”, so you must repeat costly work.</p>

<p>Alternatively, thousands of computers could each crunch through thousands of users. That’d be quicker to reach actionable results, a.k.a. revenue. However, any one of those <code>v = g(y)</code> functions could become a problem since the data for a “heavy hitter” might not fit on a single computer. Moreover, we really want to be computing more complex kinds of workflows, along the lines of <code>u = h(g(f(x)))</code> instead of just one-offs. Your blood pressure rises. You can just imagine the escalating datacenter invoices, engineering schedule delays, angry users complaining in public, plummeting revenue reports, journalists having a feeding frenzy at your expense … let alone the next board meeting with investors.</p>

<p>Relax, we can leverage abstract algebra to the rescue. There are mathematically rigorous definitions for how to split the data into chunks that our servers can handle, apply workflows of composable functions in parallel, and reassemble the results into actionable insights. Calling back to the <a href="http://en.wikipedia.org/wiki/Jerry_Maguire">Maguire film</a>, you had me at “workflows of composable functions.”</p>

<h2>Definitions</h2>

<p>Let’s start with some basic definitions for abstract algebra, from <a href="http://math.uga.edu/~pete/semigroup.pdf">Introduction to Semigroups and Monoids</a> by Pete L. Clark<a contenteditable="false" data-primary="Pete L. Clark" data-type="indexterm" id="idm2768"> </a>. To paraphrase one of the first definitions given in that paper:</p>

<blockquote>
<p><em>semigroup<a contenteditable="false" data-primary="semigroup" data-type="indexterm" id="idm74016"> </a></em> – a non-empty set with an associative binary operation</p>
</blockquote>

<p>Okay, that’s a mouthful – and brace yourself, terminology in this area of mathematics gets much, much worse. Imagine studying it for the first time from someone who spoke with a lisp in a very thick German accent. But I digress…</p>

<p>The two main properties of a semigroup are <em>associativity<a contenteditable="false" data-primary="associativity" data-type="indexterm" id="idm116496"> </a></em> and <em>closure<a contenteditable="false" data-primary="closure" data-type="indexterm" id="idm114944"> </a></em>. One example would be to consider <strong>N</strong>, the set of <em>natural numbers</em> (including zero), along with <code>+</code> the addition operator. The operation is <em>binary</em> and <em>associative</em>:</p>

<pre>
<code>2 + (3 + 4) = (2 + 3) + 4 
</code></pre>

<p>Keep in mind that the results of the operation must also be within set <strong>N</strong>. That defines the required property for <em>closure</em>.</p>

<p>Actually, using integers for examples here is not ideal. Integers have a whole bunch of other properties, which we really don’t want to confuse here. But that works for now.</p>

<p>Next, let’s consider the second definition in the <a href="http://math.uga.edu/~pete/semigroup.pdf">Clark paper</a>:</p>

<blockquote>
<p><em>monoid</em> – a semigroup with a unique identity element</p>
</blockquote>

<p>Bokay. Well, the number <code>0</code> works pretty well as an <em>identity element<a contenteditable="false" data-primary="identity element" data-type="indexterm" id="idm79200"> </a></em> for addition:</p>

<pre data-original-title="" title="">
<code>2 + 0 = 0 + 2 = 2
</code></pre>

<p>Skipping a bit further into the <a href="http://math.uga.edu/~pete/semigroup.pdf">Clark paper</a> at the bottom of page 9, we find:</p>

<blockquote>
<p><em>ring<a contenteditable="false" data-primary="ring" data-type="indexterm" id="idp34624"> </a></em> – semigroup with two binary associative operations, addition and multiplication</p>
</blockquote>

<p>We could go further, exploring concise definitions for <em>groups</em>, <em>fields</em>, etc. But let’s not – just yet.</p>

<p>Note that when we’re talking about <em>elements</em> in the context of semigroups, monoids, groups, rings, etc., these could be integer numbers… or they could be vectors<a contenteditable="false" data-primary="vector" data-type="indexterm" id="idp38336"> </a>, matrices<a contenteditable="false" data-primary="matrix" data-type="indexterm" id="idm39552"> </a>, or tensors… or polynomials… while we’re on a roll here, how about if they were <a href="http://www.slideshare.net/dgleich/the-power-and-arnoldi-methods-in-an-algebra-of-circulants">circulants</a>? Or how about if they were chunks of code? Or instances of classes in an object-oriented programming language?</p>

<p>Looking at this from a slightly different perspective, we know from arithmetic that <code>2 + 3 = 5</code>, and we know from algebra that when <code>2 + x = 5</code> then <code>x = 3</code> … great, that comes in handy. We can perhaps solve for a variable and find its correct value. However, imagine if the variable was not representing a number? What if the variable represented the operation used in the equation? Or what if the variable represented the entire set of numbers allowed in the equation? That’s quite abstract. Yes, exactly, so we call it <a contenteditable="false" data-primary="abstract algebra" data-type="indexterm" id="idm34976"> </a><em>abstract algebra</em> for a good reason. In general, this area is called <a data-original-title="" data-primary="category theory" data-secondary="" data-tertiary="" href="http://en.wikipedia.org/wiki/Category_theory" title="">Category Theory</a> – in case you feel an urge to get really abstract.</p>

<p>Why bother with this? Stated simply, it allows us to write very generic functions. More to the point, we need to leverage the algebraic properties of those generic functions to build and optimize data workflows at scale, maximizing for parallelism and ROI, minimizing latency, while also minimizing maintenance costs.</p>

<h2>Our Old Friend, MapReduce</h2>

<p>Let’s take a look at <a data-original-title="" data-primary="MapReduce" data-secondary="" data-tertiary="" href="http://arxiv.org/pdf/1304.7544v1.pdf" title="">Monoidify! Monoids as a Design Principle for Efficient MapReduce Algorithms</a> by <a data-original-title="" data-primary="Jimmy Lin" data-secondary="" data-tertiary="" href="http://www.umiacs.umd.edu/~jimmylin/" title="">Jimmy Lin</a>, U Maryland/Twitter<a contenteditable="false" data-primary="Twitter" data-type="indexterm" id="idm43248"> </a>. Kudos to Oscar Boykin, Sam Ritchie, et al.</p>

<p>We’ll begin with <strong>Algorithm 1</strong>. Consider the following simple examples written in Python… First, let’s look at the code in <code>avg_m1.py</code> for the mapper:</p>

<pre data-original-title="" title="">
<code>import sys

class Mapper (object):
    def map (self, key, val):
        print "%s\t%d" % ( key, int(val),  )

m = Mapper()

for line in sys.stdin:
    key, val = line.strip().split("\t")
    m.map(key, val)
</code></pre>

<p>Next, let’s look at the code in <code>avg_r1.py</code> for the reducer:</p>

<pre data-original-title="" title="">
<code>import sys

class Reducer (object):
    def reduce (self, key, values):
        self.total = 0
        self.count = 0

        for val in values:
            self.total += int(val)
            self.count += 1

        avg = self.total / float(self.count)
        print "%s\t%f" % ( key, avg, )

last_key = None

for line in sys.stdin:
    key, val = line.strip().split("\t")

    if not last_key == key:
        if last_key:
            r.reduce(last_key, values)

        r = Reducer()
        values = []

    values.append(val)
    last_key = key

r.reduce(key, values)
</code></pre>

<p>Given the data in <code>num.txt</code>:</p>

<pre>
<code>a   3
c   9
a   7
d   2
b   2
a   1
b   4
c   1
d   8
</code></pre>

<p>Let’s run that data through our MapReduce pipeline to calculate an average of the values for each key:</p>

<pre>
<code>$ cat num.txt | python avg_m1.py | sort | python avg_r1.py 
a   3.666667
b   3.000000
c   5.000000
d   5.000000
</code></pre>

<p>Effectively, that’s a Python implementation of <strong>Algorithm 1</strong> in the <a href="http://arxiv.org/pdf/1304.7544v1.pdf">Lin paper</a>. We could have used <a data-original-title="" data-primary="Hadoop streaming" data-secondary="" data-tertiary="" href="http://wiki.apache.org/hadoop/HadoopStreaming" title="">Hadoop streaming</a> with the same code and same data, to get the same results. For that matter, we might as well have used SQL to express this approach:</p>

<pre data-original-title="" title="">
<code>SELECT
  key, AVERAGE(val)
FROM num
GROUP BY key
;
</code></pre>

<p>But we don’t want to use SQL, because it won’t scale very well. Recall the issues with Lady Gaga’s data on Twitter: SQL is a really, <em>truly</em> excellent way to get into trouble quickly with large-scale data. Even in an tiny, early stage tech start-up, similar kinds of data issues can become show-stoppers for attempts to leverage machine learning.</p>

<p>If one uses Hadoop streaming, and one writes code like the Python code shown above, one will almost surely want to improve its performance. In other words, that kind of code runs as slow as molasses. Could we improve it by making it use of <a href="http://developer.yahoo.com/hadoop/tutorial/module4.html#functionality">combiners</a>? Or something.</p>

<p>Lin makes the point:</p>

<blockquote>
<p>This isn’t a particularly efficient algorithm because the mappers do no work and all data are shuffled (across the network) over to reducers. Furthermore, the reducer cannot be used as a combiner.</p>
</blockquote>

<p>…then delivers the punchline about monoids and parallelism:</p>

<blockquote>
<p>In general, the mean of means of arbitrary subsets of a set of values is not the same as the mean of the set of values</p>
</blockquote>

<p>Let’s skip down in the <a href="http://arxiv.org/pdf/1304.7544v1.pdf">Lin paper</a> to <strong>Algorithm 3</strong> and <strong>Algorithm 4</strong>. The code in <code>avg_m4.py</code> presents an improved mapper based on that approach:</p>

<pre>
<code>from collections import defaultdict
import sys

class Mapper (object):
    def __init__ (self):
        self.total = defaultdict(lambda: 0, {})
        self.count = defaultdict(lambda: 0, {})

    def map (self, key, val):
        self.total[key] += int(val)
        self.count[key] += 1

    def close (self):
        for key, val in self.total.items():
            print "%s\t%d\t%d" % ( key, val, self.count[key], )

m = Mapper()

for line in sys.stdin:
    key, val = line.strip().split("\t")
    m.map(key, val)

m.close()
</code></pre>

<p>The code in <code>avg_r4.py</code> presents an improved reducer based on <strong>Algorithm 3</strong> in the <a href="http://arxiv.org/pdf/1304.7544v1.pdf">Lin paper</a>:</p>

<pre data-original-title="" title="">
<code>import sys

class Reducer (object):
    def reduce (self, key, values):
        self.total = 0
        self.count = 0

        for _total, _count in values:
            self.total += int(_total)
            self.count += int(_count)

        avg = self.total / float(self.count)
        print "%s\t%f" % ( key, avg, )

last_key = None

for line in sys.stdin:
    vals = line.strip().split("\t")
    key = vals.pop(0)

    if not last_key == key:
        if last_key:
            r.reduce(last_key, values)

        r = Reducer()
        values = []

    values.append(vals)
    last_key = key

r.reduce(key, values)
</code></pre>

<p>In other words, there’s no need to define a combiner, since the mapper and reducer handle intermediate values in a much better way. Lin summarizes:</p>

<blockquote>
<p>In essence, this algorithm transforms a non-associative operation (mean of values) into an associative<a contenteditable="false" data-primary="associativity" data-type="indexterm" id="idp330032"> </a> operation (element-wise sum of a pair of numbers, with a division at the end). … we’ve created a monoid out of the intermediate value!</p>
</blockquote>

<p>Can haz monoid? Yes, can haz!</p>

<p>Jimmy Lin goes into much more detail about the use of monoids for designing MapReduce algorithms, for compiler optimization, etc. Highly recommended. In particular, he mentions about <em>stochastic gradient descent<a contenteditable="false" data-primary="stochastic gradient descent" data-type="indexterm" id="idp333184"> </a></em> used to train classifiers, with the model parameter as a monoid – please keep that in mind for later! In general, he references about the <a data-original-title="" data-primary="Algebird" data-secondary="" data-tertiary="" href="https://github.com/twitter/algebird" title="">Algebird</a> open source project from Twitter<a contenteditable="false" data-primary="Twitter" data-type="indexterm" id="idp336672"> </a>, used by <a data-original-title="" data-primary="Scalding" data-secondary="" data-tertiary="" href="https://github.com/twitter/scalding" title="">Scalding</a>, <a data-original-title="" data-primary="Spark" data-secondary="" data-tertiary="" href="http://spark.incubator.apache.org/" title="">Spark</a>, <a data-original-title="" data-primary="Summingbird" data-secondary="" data-tertiary="" href="https://github.com/twitter/summingbird" title="">Summingbird</a>, etc. We’ll come back to Algebird in a moment.</p>

<h2>Curried Lambda Stew</h2>

<p>As all good Star Trek fans know, Data is intended to be <em>fully functional</em>. Digging into the origins of <em>functional programming<a contenteditable="false" data-primary="functional programming" data-type="indexterm" id="idm154544"> </a></em>, we find plenty of references to <a data-original-title="" data-primary="Haskell Curry" data-secondary="" data-tertiary="" href="http://en.wikipedia.org/wiki/Haskell_Curry" title="">Haskell Curry</a>, known for seminal work on <a href="http://www.haskell.org/haskellwiki/Combinatory_logic">combinatory logic</a> (1927), and to <a data-original-title="" data-primary="Alonzo Church" data-secondary="" data-tertiary="" href="http://en.wikipedia.org/wiki/Alonzo_Church" title="">Alonzo Church</a>, known for <a data-original-title="" href="http://plato.stanford.edu/entries/lambda-calculus/" title="">lambda calculus</a> (1936) and much more! Both of those formulations sought to define formal answers to the question “What can be computed?” The work of Curry and Church, respectively, helped inform a wide range computer science research, and gave rise to programming languages such as <a href="http://xkcd.com/224/">LISP</a>.</p>

<p><a href="http://youtu.be/bHcN4Gm8tzM"><img alt="Data" src="http://img.youtube.com/vi/bHcN4Gm8tzM/0.jpg"/></a></p>

<p>Rolling the clock out to 1977, John Backus<a contenteditable="false" data-primary="John Backus" data-type="indexterm" id="idm194640"> </a> incorporated those notions and more into a relatively “pure” form of functional programming, presented in the award-winning paper, <a href="http://www.stanford.edu/class/cs242/readings/backus.pdf">Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs</a>. On page 619, we find a key point articulated quite clearly:</p>

<blockquote>
<p>The FP algebra is compared with algebras associated with the classical applicative systems of Church and Curry.</p>
</blockquote>

<p>Backus was quite interested in, among other things, how to increase the parallelism of programs, moving away from <a data-original-title="" data-primary="Von Neumann architecture" data-secondary="" data-tertiary="" href="http://en.wikipedia.org/wiki/Von_Neumann_architecture" title="">Von Neumann architecture</a>. He summarized four key elements of his approach:</p>

<ul>
	<li>a functional style of programming without variables</li>
	<li>an algebra of functional programs</li>
	<li>a formal functional programming system</li>
	<li>applicative state transition<a contenteditable="false" data-primary="applicative state transition" data-type="indexterm" id="idm186544"> </a> systems</li>
</ul>

<p>Then skipping to page 638:</p>

<blockquote>
<p>Programs can be analyzed and optimized by an algebra of programs…</p>

<p>Since the state cannot change during the computation of system:x, there are no side effects. Thus independent applications can be evaluated in parallel.</p>
</blockquote>

<p>Good stuff, straight to our point from the examples discussed above, and well worth a careful reading. However, it’s not immediately useful in practice since you won’t find many FP-based open source projects on GitHub.</p>

<p>Speaking of applicatives… A decade or so later in 1990, other research efforts converged on an open standard for functional programming called <a href="http://www.haskell.org/haskellwiki/Haskell">Haskell</a>. Arguably, this language put into practice much of what Backus had envisioned. Let’s take a good look at a recommended text, <a href="http://learnyouahaskell.com/functors-applicative-functors-and-monoids">Learn You a Haskell for Great Good!</a> by Miran Lipovača<a contenteditable="false" data-primary="Miran Lipovača" data-type="indexterm" id="idm180192"> </a>.</p>

<p>In contrast to our Python examples, Haskell shows how a strongly typed language approaches these algebraic abstractions. The type system itself is where the math shows up most directly. Lipovača starts out with type classes like functors and applicatives, then moves into monoids<a contenteditable="false" data-primary="monoid" data-type="indexterm" id="idm178208"> </a>. Again, the definition is an associative, binary operation on a set, with an identity element<a contenteditable="false" data-primary="identity element" data-type="indexterm" id="idm176864"> </a>:</p>

<ul>
	<li>the function takes two parameters</li>
	<li>the parameters and the returned value have the same type</li>
	<li>there exists such a value that doesn’t change other values when used with the binary function</li>
</ul>

<p>Continuing on, the <a data-original-title="" href="http://learnyouahaskell.com/functors-applicative-functors-and-monoids" title="">Lipovača article</a> discusses how to handle edge cases with <code>Maybe</code>… how to perform <em>maps</em> and <em>folds<a contenteditable="false" data-primary="folds" data-type="indexterm" id="idp859760"> </a></em>… how to apply monoids for work with lists, trees, etc. Keep in mind that graphs can be represented as trees or sparse matrices, and that most real-world problems in large-scale data involve some kind of graphs. Since monoids provide robust, mathematical definitions for how to compute parts of the data in chunks and how to handle “empty” elements, they tend to handle large, sparse matrices<a contenteditable="false" data-primary="matrix" data-type="indexterm" id="idp861440"> </a> quite well.</p>

<h2>The Aviary</h2>

<p>Lipovača’s examples in Haskell set the stage for how abstract algebra gets used in Scala, namely with <a data-original-title="" data-primary="Algebird" data-secondary="" data-tertiary="" href="https://github.com/twitter/algebird" title="">Algebird</a>, <a data-original-title="" data-primary="Summingbird" data-secondary="" data-tertiary="" href="https://github.com/twitter/summingbird" title="">Summingbird</a>, etc. As it turns out, there’s been a long history of associating <a data-original-title="" data-primary="combinators" data-secondary="" data-tertiary="" href="http://www.angelfire.com/tx4/cus/combinator/birds.html" title="">birds and combinators</a>, tracing to <a data-original-title="" data-primary="Raymond Smullyan" data-secondary="" data-tertiary="" href="http://en.wikipedia.org/wiki/To_Mock_a_Mockingbird" title="">Raymond Smullyan</a>:</p>

<blockquote>
<p>Each bird has a distinctive call, which it emits when it hears the call of another bird.</p>
</blockquote>

<p>Quite fitting, given how much Twitter has invested in this area.</p>

<p>Before we jump into Scala, first let’s take a look at another foundational component. Rolling the clock up to late 2007, Chris Wensel<a contenteditable="false" data-primary="Chris Wensel" data-type="indexterm" id="idp874144"> </a> introduced a Java API called <a href="http://www.cascading.org/">Cascading</a>, as a way to build Enterprise data workflows<a contenteditable="false" data-primary="data workflows" data-type="indexterm" id="idp876128"> </a> atop Hadoop. Rather than write code directly using the MapReduce API in Hadoop, programmers could use aspects of functional programming languages directly within Java to define workflows.</p>

<p>Thanks to a <a href="http://en.wikipedia.org/wiki/Separation_of_concerns">separation of concerns</a> between the business logic of workflows versus the nuts and bolts of Hadoop<a contenteditable="false" data-primary="Hadoop" data-type="indexterm" id="idp878800"> </a>, Cascading<a contenteditable="false" data-primary="Cascading" data-type="indexterm" id="idp880032"> </a> leveraged – you guessed it – notions about functors, applicatives, monoids, etc. Those aspects of Cascading are not called out explicitly, but they allow it to apply optimization techniques from query engines, compilers, etc., and provide a library of very generic functions.</p>

<p><a href="https://github.com/Cascading/Impatient/tree/master/part4"><img alt="Cascading workflow" src="http://www.cascading.org/files/2012/08/plumb4.png"/></a></p>

<p>Another aspect of Cascading which is not called out explicitly is its use of a<br/>
<a href="http://instiki.cs242.vazexqi.com/publications/barrier.pdf">barrier pattern</a>, as a synchronization method for parallel computing. Pipes get defined by composing functions, handling our case of <code>u = h(g(f(x)))</code> quite nicely. Some functions get applied to the pipes at special points, for example immediately following a <code>GroupBy</code> aggregation as shown in the diagram. That implies that all of the preceding tasks have completed. In other words, map tasks run in parallel <em>before</em> the barrier, and the <code>GroupBy</code> which comes <em>after</em> the barrier in turn relies on those results. More about that in a moment.</p>

<blockquote> </blockquote>

<p>This brings us to back to Scala. Rolling the clock up to 2012, Twitter introduced <a href="https://github.com/twitter/scalding/wiki">Scalding</a> a Scala API for Cascading. Twitter subsequently reworked their revenue apps based on Scalding and has been <a href="https://engineering.twitter.com/opensource">evangelizing</a> its adoption – now deployed at eBay, LinkedIn, etc.</p>

<p>Another Scala library from Twitter is called <a href="https://engineering.twitter.com/opensource/projects/algebird">Algebird</a> which provides <a href="https://github.com/twitter/algebird/wiki/Abstract-algebra-definitions">abstract algebra definitions</a> for Scalding, Storm, Spark, etc. Its release sparked lots of discussion: why would Twitter be interested in abstract algebra? See some great answers on <a href="http://www.quora.com/Twitter-1/What-is-Twitters-interest-in-abstract-algebra-with-algebird">Quora</a> and <a data-original-title="" href="http://cs.stackexchange.com/questions/9648/what-use-are-groups-monoids-and-rings-in-database-computations" title="">StackExchange</a>. Oscar Boykin<a contenteditable="false" data-primary="Oscar Boykin" data-type="indexterm" id="idp120944"> </a>, one of the main authors of these Twitter projects, explains:</p>

<blockquote>
<p>The main answer is that by exploiting semi-group structure, we can build systems that parallelize correctly without knowing the underlying operation (the user is promising associativity).</p>

<p>By using Monoids, we can take advantage of sparsity (we deal with a lot of sparse matrices, where almost all values are a zero in some Monoid).</p>

<p>By using Rings, we can do matrix multiplication over things other than numbers (which on occasion we have done).</p>

<p>The algebird project itself (as well as the issue history) pretty clearly explains what is going on here: we are building a lot of algorithms for aggregation of large data sets, and leveraging the structure of the operations gives us a win on the systems side (which is usually the pain point when trying to productionize algorithms on 1000s of nodes).</p>

<p>Solve the systems problems once for any Semigroup/Monoid/Group/Ring, and then you can plug in any algorithm without having to think about Memcache, Hadoop, Storm, etc...</p>
</blockquote>

<p>Let’s take a quick look at one of the better articles about Algebird, written by <a href="http://www.michael-noll.com/blog/2013/12/02/twitter-algebird-monoid-monad-for-large-scala-data-analytics/">Michael Noll</a>. Noll was interested to explore Algebird, monoid, monads, etc., and wrote this tutorial – which reads a bit like a journal. He shows a great example, which references back to our Twitter use case mentioned above, contending with “heavy hitters”<a contenteditable="false" data-primary="heavy hitters" data-type="indexterm" id="idp127312"> </a>:</p>

<pre>
<code>// Let's have a popularity contest on Twitter.
// The user with the most followers wins!
val barackobama = TwitterUser("BarackObama", 40267391)
val katyperry = TwitterUser("katyperry", 48013573)
val ladygaga = TwitterUser("ladygaga", 40756470)
val miguno = TwitterUser("miguno", 731) // I participate, too.  Olympic spirit!
val taylorswift = TwitterUser("taylorswift13", 37125055)

val winner: Max[TwitterUser] = Max(barackobama) + Max(katyperry) + Max(ladygaga) + Max(miguno) + Max(taylorswift)
assert(winner.get == katyperry)
</code></pre>

<p>Katy Perry wins, with <code>48,013,573</code> followers. This approach also hints at being able to distribute algorithms, especially for estimates. We’ll come back to that in another section.</p>

<p>Looking at the section titled “Monoids”, we see a familiar discussion about associativity, closure, identity element, fold, etc. Note how these definitions get implemented Scala by leveraging the type system. The source code on GitHub shows some good examples, e.g., where <code>Monoid</code> subclasses <code>Semigroup</code>:</p>

<pre>
<code>trait Monoid[@specialized(Int,Long,Float,Double) T] extends Semigroup[T] {
  def zero : T //additive identity
  def isNonZero(v: T): Boolean = (v != zero)
  def assertNotZero(v : T) {
    if(!isNonZero(v)) {
      throw new java.lang.IllegalArgumentException("argument should not be zero")
    }
  }

  def nonZeroOption(v : T): Option[T] = {
    if (isNonZero(v)) {
      Some(v)
    }
    else {
      None
    }
  }

  // Override this if there is a more efficient means to implement
  def sum(vs: TraversableOnce[T]): T = sumOption(vs).getOrElse(zero)
}
</code></pre>

<p>Looking at the section titled “Monads” in the <a href="http://www.michael-noll.com/blog/2013/12/02/twitter-algebird-monoid-monad-for-large-scala-data-analytics/">Noll article</a>, we see <em>monads</em> as a kind of structure used for defining chains of functions. In other words, our friend <code>u = h(g(f(x)))</code> gets used to define data workflows. Note that this way of specifying the business logic of a workflow is expressed quite explicitly in algebra, in contrast to Cascading. Also, it applies beyond Hadoop – that is, Scalding atop Cascading atop Hadoop. Much the same code can be used either in Scalding (batch) or in Storm (real-time).</p>

<blockquote> </blockquote>

<p>That leads us into a more recent Twitter project, released in 2013, called <a href="https://github.com/twitter/summingbird/wiki">Summingbird</a>. Wired magazine covered <a href="http://www.wired.com/wiredenterprise/2013/11/twitter-summingbird/">a story</a> with two of its principal authors, Oscar Boykin and Sam Ritchie. The notion is to leverage the abstract algebra definitions for the business logic of data workflows, relatively independent of the distributed frameworks underneath:</p>

<blockquote>
<p>“Summingbird can describe logic that can run in real-time or on Hadoop or just on your laptop,” Boykin says. “You can run it in all these different places without having to worry too much about each one, and you can then combine all the results.”</p>
</blockquote>

<p>Back to the note above about using a barrier pattern… Apache Hadoop provides a popular framework for large-scale computing. However, if you have Summingbird and Algebird defining your workflows, that opens up some alternatives.</p>

<p>Suppose that you have some “building block” with which you can readily assemble new kinds of distributed frameworks to provide barrier patterns, provide a distributed file system, manage tasks and services in a fault-tolerant way on commodity hardware, etc. That would obviate the need for Hadoop. Looking back again at the Twitter open source page, notice that the <a href="https://engineering.twitter.com/opensource/projects/mesos">Mesos project</a> is listed alongside Summingbird. The needed features described above can be assembled in about 100 lines of Scala, using the Mesos building blocks. Compare that with over <a href="http://hortonworks.com/blog/reality-check-contributions-to-apache-hadoop/">1 million lines of source code</a> contributed to Apache Hadoop since 2006, and growing. Words to the wise.</p>

<h2>Monoids in Python</h2>

<p>Let’s shift back to Python for another coding example. Take a look at the article <a href="http://fmota.eu/blog/monoids-in-python.html">Monoids in Python</a> by Francisco Mota. The definitions <em>associativity</em>, <em>closure</em>, and <em>identity element</em> discussed there should be familiar by now.</p>

<p>Mota defines a monoid in Python similar to how Algebird does the same in Scala – minus Scala’s <a href="http://typesafe.com/platform/tools/scala">type safety</a>:</p>

<pre data-original-title="" title="">
<code>class Monoid:
    def __init__(self, null, lift, op):
        self.null = null
        self.lift = lift
        self.op   = op

    def __call__(self, *args):
        result = self.null
        for arg in args:
            arg = self.lift(arg)
            result = self.op(result, arg)
        return result
</code></pre>

<p>Leveraging this class, the <code>summ</code> monoid defines how to add a list of integers:</p>

<pre>
<code>summ = Monoid(0, lambda x: x, lambda a,b: a+b)
</code></pre>

<p>To put this into use:</p>

<pre>
<code>summ(10, 20, 30) == 10 + 20 + 30 == 60
</code></pre>

<p>Adding numbers together, got it. Let’s try with a more complex data structure – dictionaries. The following code comes from <a href="https://github.com/ceteri/exelixi/blob/master/src/monoids.py">Exelixi</a>:</p>

<pre>
<code>dictm = Monoid({}, lambda x: x, lambda a,b: dict_op(a, b))

def dict_op (a, b):
    for key, val in b.items():
        if not key in a:
            a[key] = val
        else:
            a[key] += val
    return a
</code></pre>

<p>Okay, let’s run through the numbers… Notice that <code>dictm</code> uses an empty dictionary <code>{}</code> for the identity element. Check! It uses <code>lambda x: x</code> for the “lift” method – no type conversion required there, closure is simple in this case. Check! Then it uses <code>dict_op(a, b)</code> as a helper method to perform the associative binary operation – adding elements into the dictionary. Check! Here’s an example usage:</p>

<pre data-original-title="" title="">
<code>&gt;&gt;&gt; from monoids import dictm
&gt;&gt;&gt; x1 = { "a": 2, "b": 3 }
&gt;&gt;&gt; x2 = { "b": 2, "c": 7 }
&gt;&gt;&gt; print x1, x2
{'a': 2, 'b': 3} {'c': 7, 'b': 2}
&gt;&gt;&gt; print dictm.fold([x1, x2])
{'a': 2, 'c': 7, 'b': 5}
&gt;&gt;&gt; 
</code></pre>

<p>Great, now we have a monoid for dictionaries – and therefore, a way to combine the results of distributed processing which get expressed as dictionaries. That’s one of the most commonly used data structures in Python. Hint: if you just <em>happened</em> to have a distributed data structure in Python running across a cluster, and you just <em>happened</em> to have a barrier pattern implemented to synchronize parallel tasks, then using <code>dictm</code> or other monoids would really help aggregate results. No Hadoop required.</p>

<p>We’ll come back to this later, when we consider more advanced data structures used for <a href="http://blog.aggregateknowledge.com/2012/10/25/sketch-of-the-day-hyperloglog-cornerstone-of-a-big-data-infrastructure/">streaming large amounts of data</a>.</p>

<blockquote> </blockquote>

<p>Oh, and let’s not give the impression that this kind of work is limited to Scala and Python. There are plenty of other excellent frameworks based on other environments. For example, check out <a href="http://plosworkshop.org/2013/preprint/dzik.pdf">MBrace: Cloud Computing with Monads</a> by Jan Dzik, et al., for F# and .NET on Windows:</p>

<blockquote>
<p>MBrace is a distributed programming model and framework based on the .NET software stack. The programming model of MBrace is founded on F#’s computation expressions. Parallelism patterns are introduced using primitive combinators that act on cloud work- ﬂows. Cloud workflows are executed by a scheduler running on the MBrace runtime, which transparently allocates cluster resources and ensures fault tolerance.</p>
</blockquote>

<p>Also, there is <a href="https://github.com/avibryant/simmer">Simmer</a> for running aggregations based on Algebird as Linux/Unix command line utilities. No. Hadoop. Required.</p>

<blockquote> </blockquote>

<hr/>
<h2>Key Points</h2>

<p>The following key points have been abstracted from the various papers and articles linked above:</p>

<ul>
	<li>Abstract algebra combined with functional programming allow for efficient, reusable code.</li>
	<li>By exploiting semigroup structure, we can build systems that can be parallelized correctly without even knowing the operations involved.</li>
	<li>Monoids take advantage of sparsity, such as in sparse matrices where almost all of the values will be a zero in some monoid.</li>
	<li>Solve the systems problems once for any semigroup, monoid, group, ring, field, etc., then plug in any algorithm without having to consider about Hadoop, Storm, etc.</li>
	<li>Monoids are composable, and therefore great for defining the business logic of large-scale data workflows.</li>
</ul>

<h2>Suggested Books</h2>

<p><em>To Mock a Mockingbird: And Other Logic Puzzles</em><br/>
by Raymond Smullyan (2000)<br/>
<a href="http://amazon.com/dp/0192801422">http://amazon.com/dp/0192801422</a></p>

<p><em>Programming Scala: Scalability = Functional Programming + Objects</em><br/>
by Dean Wampler, Alex Payne (2009)<br/>
<a data-original-title="" href="http://amazon.com/dp/0596155956" title="">http://amazon.com/dp/0596155956</a></p>

<p><em>Learn You a Haskell for Great Good!</em><br/>
by Miran Lipovača (2011)<br/>
<a href="http://amazon.com/dp/1593272839">http://amazon.com/dp/1593272839</a></p>

<p><em>Enterprise Data Workflows with Cascading</em><br/>
by Paco Nathan (2013)<br/>
<a href="http://amazon.com/dp/1449358721">http://amazon.com/dp/1449358721</a></p>

<p><em>Functional Programming in Scala</em><br/>
by Paul Chiusano, Runar Bjarnason (2014)<br/>
<a href="http://amazon.com/dp/1617290653">http://amazon.com/dp/1617290653</a></p>

<h2>Exercises</h2>

<ol>
	<li>Write a monoid in Python for <em>set union</em>, and show it running.</li>
	<li>Can you point out any semigroups or monoids described in the <a href="http://www.stanford.edu/class/cs242/readings/backus.pdf">Backus paper</a>?</li>
</ol>
</section>
  </body>
</html>
