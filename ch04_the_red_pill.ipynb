{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The Red Pill"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Topics:** _Linear Algebra_, _Linear Systems_, _Least Squares_, _Eigenvalues_, _Adjacency Matrix_, _Dimensional Reduction_, _Low-Order Embedding_, _Sparse Matrices_, _Algebraic Graph Theory_, _Graph Queries_, _Parallel Processing_"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Linear Algebra is a very broad and encompassing subject in mathematics, with quite a [long history](http://jeff560.tripod.com/matrices.html), dating back to [Babylonian, and possibly Ancient Egyptian](http://it.stlawu.edu/~dmelvill/mesomath/erbiblio.html#Friberg) sources. It is difficult to find any other part of mathematics that does not rely on linear algebra. Apropos, a Hollywood trope of \u201cthe red pill\u201d traces through [The Matrix](http://www.imdb.com/title/tt0133093/) (1999), [Total Recall](http://www.imdb.com/title/tt0100802/) (1990), and probably originated from a [Phillip K. Dick story](http://en.wikipedia.org/wiki/We_Can_Remember_It_for_You_Wholesale) (1966). It signifies both an embrace of reality and a call to action. A mathematician couldn\u2019t ask for better."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Definitions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s start with the notion of a _vector_, which is simply a one-dimensional array of values:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "x = [ 2, 3, 1, 0 ]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we\u2019ll contrast that with the notion of a _scalar_, which is simply a value:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "a = 3\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An addition operation performed on a vector and a scalar produces another vector:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "a + x = [ 3 + 2, 3 + 3, 3 + 1, 3 + 0 ] = [ 5, 6, 4, 3 ]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "can't assign to operator (<ipython-input-3-2e3bc8872dc5>, line 2)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-2e3bc8872dc5>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    a + x = [ 3 + 2, 3 + 3, 3 + 1, 3 + 0 ] = [ 5, 6, 4, 3 ]\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to operator\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or we might do a multiplication to produce another vector:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "b = 2\n",
      "b * x "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[2, 3, 1, 0, 2, 3, 1, 0]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or we could add two vectors together to produce another vector:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "y = [ 5, 1, 5, 0 ]\n",
      "x + y = [ 2 + 5, 3 + 1, 1 + 5, 0 + 0 ] = [ 7, 4, 6, 0 ]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall from the basic definitions in abstract algebra:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The closure property is here \u2013 gotcha, and there are binary associative operations for addition and multiplication \u2013 gotcha. This is familiar turf. Check out the definition of an [abstract vector space](http://empg.maths.ed.ac.uk/xmlearning/lecture_notes/vector_spaces/abstract_vector_spaces/abstract_vector_spaces.php), which begins to bridge between abstract algebra and linear algebra."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s push beyond that \u2013 let\u2019s multiply two vectors. There are a couple ways to do that. First, we can use a _dot product_, sometimes called an _inner product_, to produce a scalar:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "x = [ 2, 3, 1, 0 ]\n",
      "y = [ 5, 1, 5, 0 ]\n",
      "x \uff65 y = (2 * 5) + (3 * 1) + (1 * 5) + (0 * 0) = 10 + 3 + 5 + 0 = 18\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Python, we can use the `linalg` package in [NumPy](http://docs.scipy.org/doc/numpy/reference/routines.linalg.html) to perform a dot product \u2013 using combinations of both vectors and scalars:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import numpy.linalg as np\n",
      "x = [ 2, 3, 1, 0 ]\n",
      "y = [ 5, 1, 5, 0 ]\n",
      "print np.dot(x, y)\n",
      "\n",
      "\n",
      "a = 3\n",
      "b = 2\n",
      "print np.dot(a, b)\n",
      "6\n",
      "\n",
      "print np.dot(b, x)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'module' object has no attribute 'dot'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-11-385b4ff62802>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'dot'"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let\u2019s consider the notion of a _matrix_ \u2013 not the blockbuster movie franchise, but the thing that was a spreadsheet long before spreadsheets became [Apple II software](http://apple2history.org/history/ah18/#07). We\u2019ll define a matrix **A** as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>1</td>\n",
      "\t\t\t<td>2</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>3</td>\n",
      "\t\t\t<td>4</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then another matrix **B** as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>5</td>\n",
      "\t\t\t<td>6</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>7</td>\n",
      "\t\t\t<td>8</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Python, we can \u201creshape\u201d an array into a matrix, then perform matrix addition and other operations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      ">>> import numpy as np\n",
      ">>> A = np.array([1, 2, 3, 4]).reshape(2, 2)\n",
      ">>> A\n",
      "array([[1, 2],\n",
      "       [3, 4]])\n",
      "\n",
      ">>> B = np.array([5, 6, 7, 8]).reshape(2, 2)\n",
      "\n",
      ">>> np.add(A, B)\n",
      "array([[ 6,  8],\n",
      "       [10, 12]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pretty much all of this code shown in Python can be done in Scala using the [Matrix API](https://github.com/twitter/scalding/wiki/Matrix-API-Reference) from Twitter \u2013 with the added benefit of getting to apply the monoids, etc., from the earlier chapter. Python is simpler to show in small examples, so we\u2019ll stick with that."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let\u2019s introduce some notation to reference the elements in matrix **A**:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>A<sub>11</sub>\n",
      "</td>\n",
      "\t\t\t<td>A<sub>12</sub>\n",
      "</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>A<sub>21</sub>\n",
      "</td>\n",
      "\t\t\t<td>A<sub>22</sub>\n",
      "</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given our definition of the values in matrix **A** and matrix **B** above, that\u2019s another way of stating that A11 = 1, A12 = 2, A21 = 3, A22 = 4, as well as B11 = 5, B12 = 6, B21 = 7, B22 = 8 \u2026 just in a much more compact form."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use a similar notation for vectors:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody><tr>\n",
      "<td>x<sub>11</sub>\n",
      "</td>\n",
      "\t\t\t<td>x<sub>12</sub>\n",
      "</td>\n",
      "\t\t</tr></tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a new definition of the values in vector **x** = [ 2, 3 ], that\u2019s another way of stating: x1 = 2, x2 = 3, just in a much more compact form."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can multiply matrix **A** and vector **x**, stated as **Ax**, based on using a sequence of dot products for each of the elements in the resulting matrix:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[ [ A11, A12 ] \uff65 [ x1, x2 ] , [ A21, A22 ] \uff65 [ x1, x2 ] ]  \n",
      "= [ (A11 \\* x1 + A12 \\* x2), (A21 \\* x1 + A22 \\* x2) ]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using our example:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[ [ 1, 2 ] \uff65 [ 2, 3 ] , [ 3, 4 ] \uff65 [ 2, 3 ] ]  \n",
      "= [ (1 \\* 2 + 2 \\* 3), (3 \\* 2 + 4 \\* 3) ]  \n",
      "= [ 2 + 6, 6 + 12 ]  \n",
      "= [ 8, 18 ]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In other words, we could solve for an equation **Ax** = **y**, where **y** = [ 8, 18 ] in this case. Using the NumPy package in Python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      ">>> import numpy as np\n",
      ">>> A = np.array([1, 2, 3, 4]).reshape(2, 2)\n",
      ">>> x = [2, 3]\n",
      ">>> np.dot(A, x)\n",
      "array([8, 18])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extending this notion, we can multiply matrix **A** and matrix **B**. To perform that operation in Python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      ">>> A = np.array([1, 2, 3, 4]).reshape(2, 2)\n",
      ">>> B = np.array([5, 6, 7, 8]).reshape(2, 2)\n",
      ">>> np.dot(A, B)\n",
      "array([[19, 22],\n",
      "       [43, 50]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the definition of a _monoid_ as a _semigroup_ with a unique identity element. We define an _identity matrix_ **I**, which has the value `1` in the diagonal and `0` for all other elements:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>1</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>0</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use this for matrix multiplication, such that **AI** = **A**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      ">>> import numpy as np\n",
      ">>> A = np.array([1, 2, 3, 4]).reshape(2, 2)\n",
      ">>> I = np.array([1, 0, 0, 1]).reshape(2, 2)\n",
      ">>> np.dot(A, I)\n",
      "array([[1, 2],\n",
      "       [3, 4]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One more definition before moving back into reality\u2026 We denote the _determinant_ of a matrix **A** as _det_(**A**) or |**A**|, which gets used in many different ways. Referencing the elements in matrix **A**:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>A<sub>11</sub>\n",
      "</td>\n",
      "\t\t\t<td>A<sub>12</sub>\n",
      "</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>A<sub>21</sub>\n",
      "</td>\n",
      "\t\t\t<td>A<sub>22</sub>\n",
      "</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then _det_(**A**) is defined as A11 \\* A22 - A12 \\* A21, and using the values we defined earlier:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A11 \\* A22 - A12 \\* A21  \n",
      "= 1 \\* 4 - 2 \\* 3  \n",
      "= 4 - 6  \n",
      "= \u20132"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the NumPy package in Python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      ">>> np.linalg.det(A)\n",
      "-2.0\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Linear Systems"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Back to the red pill, and our call to action to embrace reality\u2026 let\u2019s put some linear algebra into practice."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we define a function `f(x)` , where:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "f(x) = x + 9\n",
      "f(2) = 2 + 9 = 11\n",
      "f(21) = 21 + 9 = 23\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great. Now let\u2019s apply this to a vector **x**, where:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "x = [ 2, 3 ]\n",
      "f(x) = [ f(2), f(3) ] = [ 2 + 9, 3 + 9 ] = [ 11, 12 ]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bokay, now we\u2019re applying a function to a vector. That could probably come in quite handy for parallel processing at scale. Or something. Moreover, we could do much the same to a matrix."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let\u2019s take a look at a common problem in algebra \u2013 solving for two equations and two unknowns:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3 x1 + 9 x2 = 5  \n",
      "4 x1 + 8 x2 = 12"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Substituting for x1, we can rearrange one of these equations to simplify the problem:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4 x1 + 8 x2 = 12  \n",
      "x1 + 2 x2 = 3  \n",
      "x1 = \u20132 x2 + 3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3 x1 + 9 x2 = 5  \n",
      "3 \\* (\u20132 x2 + 3) + 9 x2 = 5  \n",
      "\u20136 x2 + 9 + 9 x2 = 5  \n",
      "(9 - 6) x2 = 5 - 9  \n",
      "3 x2 = \u20134  \n",
      "x2 = \u20134/3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, substituting this known variable to solve for the remaining unknown variable:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "x2 = \u20134/3  \n",
      "x1 = \u20132 x2 + 3  \n",
      "x1 = (\u20132 \\* \u20134/3) + 3 = 8/3 + 3 = 5 2/3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Be sure to check our math:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(3 \\* 5 2/3) + (9 \\* \u20134/3) = 17 - 12 = 5  \n",
      "(4 \\* 5 2/3) + (8 \\* \u20134/3) = 22 2/3 - 10 2/3 = 12"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good, that worked. What a relief. Generally speaking, if we have _N_ equations and _N_ variables, we can solve for that _system of equations_. When we have more or less than _N_ equations, life becomes interesting. In any case, we can use linear algebra to make this a bit simpler. The same problem could have been stated as one equation **Ax** = **y**, where the matrix **A** is:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>3</td>\n",
      "\t\t\t<td>9</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>4</td>\n",
      "\t\t\t<td>8</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and **y** = [ 5, 12 ], and then we just need to solve for one variable, vector **x**. Given the equation **Ax** = **y**, how about if we simply divide by **A** to get the answer? Rather than divide by **A**, we must multiply both sides of the equation by its [matrix inverse](http://mathworld.wolfram.com/MatrixInverse.html). That is denoted as **A**\u20131, such that **AA**\u20131 = **I** holds."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finding the matrix inverse can be tricky business. In general, to find **A**\u20131 we take the rearranged elements of **A**:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>A<sub>22</sub>\n",
      "</td>\n",
      "\t\t\t<td>-A<sub>12</sub>\n",
      "</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>A<sub>-21</sub>\n",
      "</td>\n",
      "\t\t\t<td>A<sub>11</sub>\n",
      "</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we divide by the determinant _det_(**A**), which is a scalar. Using the value for matrix **A** given in our system of equations, the rearranged elements become:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>8</td>\n",
      "\t\t\t<td>-9</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>-4</td>\n",
      "\t\t\t<td>3</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then dividing by the determinant _det_(**A**) = \u201312, we get:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>-2/3</td>\n",
      "\t\t\t<td>3/4</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>1/3</td>\n",
      "\t\t\t<td>-1/4</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the NumPy package in Python is a much simpler way to find **A**\u20131 and solve for **x**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      ">>> import numpy as np\n",
      ">>> A = np.array([3, 9, 4, 8]).reshape(2, 2)\n",
      ">>> np.linalg.det(A)\n",
      "-12.0\n",
      "\n",
      ">>> invA = np.linalg.inv(A)\n",
      ">>> invA\n",
      "array([[-0.66666667,  0.75],\n",
      "       [0.33333333, -0.25]])\n",
      "\n",
      ">>> y = [ 5, 12 ]\n",
      ">>> np.dot(invA, y)\n",
      "array([5.66666667, -1.33333333])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That looks right. Frankly, this a small example. These kinds of [linear systems](http://www.numbertheory.org/book/cha1.pdf) in practice may grow to be very, very large. Finding a matrix inverse in those cases can be quite costly and can become problematic in other ways \u2013 especially for approximations, which sometimes are the only feasible approach at scale. So we try to find ways of performing the math without calculating an inverse. More about that in a bit."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Least Squares Approximation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that when we have more or less than _N_ equations with _N_ variables, life becomes interesting. We will consider both cases in this chapter, but let\u2019s start with the _more_ case."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Imagine that we have many points `(x, y)` on a 2-D grid. We could use linear algebra to describe a line that runs through those points. The general formula of a line is `y = mx + c` , with some scalar values `m` and `c` as the slope and intercept of the line, respectively. If all the points are aligned, it should be no problem solving for those values, and we\u2019ll get an exact solution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, if all the points are not aligned \u2013 and worse yet if there are too many of them, i.e., more equations than variables \u2013 there won\u2019t be an exact solution, but we can approximate. In the case of a 2-D grid, any more than two points can cause this condition. The solution involves _curve fitting_, and _least squares approximation_ was introduced for by [Gauss](http://jeff560.tripod.com/m.html) in the early 19th century. Check out the discussion about [least squares approximation](http://math.mit.edu/linearalgebra/ila0403.pdf \"\") by Gilbert Strang, in the book _Introduction to Linear Algebra_."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s say that our list of `(x, y)` points to be fitted is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "(0, -1), (1, 0.2), (2, 0.9), (3, 2.1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We define the vector **x**T = [m, c] to solve for the line parameters. We construct the matrix **A** with all the points\u2019 `x` values in the first column, and the value `1` in the second column:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>0</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>1</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>2</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>3</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we define the vector **y** with the points\u2019 `y` values = [\u20131, 0.2, 0.9, 2.1], and now we can solve for the equation **Ax** = **y**. Since matrix **A** is not square, we cannot obtain a matrix inverse."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead we use a [transpose](http://mathworld.wolfram.com/Transpose.html) of matrix **A**, denoted at **A**T, where the elements get reflected along the diagonal:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>A<sub>11</sub>\n",
      "</td>\n",
      "\t\t\t<td>A<sub>21</sub>\n",
      "</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>A<sub>12</sub>\n",
      "</td>\n",
      "\t\t\t<td>A<sub>22</sub>\n",
      "</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that (**A**T)T = **A** holds, and moreover the transpose works fine even when the matrix is not square."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we multiply both sides of **Ax** = **y** by **A**T, then approximate **x** in the resulting **A**T**Ax** = **A**T**y** equation. To approximate **x**, we split the vector into two parts: a projection **p** = [ m, c ] as the solution, and a vector of errors **e**. Then we minimize **e** as best as possible by minimizing the squares of its values, ergo the \u201cleast squares\u201d name."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the [linalg.lstsq](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html \"\") method in NumPy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      ">>> import numpy as np\n",
      ">>> x = np.array([0, 1, 2, 3])\n",
      ">>> y = np.array([-1, 0.2, 0.9, 2.1])\n",
      ">>> A = np.vstack([x, np.ones(len(x))]).T\n",
      ">>> A\n",
      "array([[0.,  1.],\n",
      "       [1.,  1.],\n",
      "       [2.,  1.],\n",
      "       [3.,  1.]])\n",
      ">>> m, c = np.linalg.lstsq(A, y)[0]\n",
      ">>> m, c\n",
      "(1.0, -0.95)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bueno. The fitted line is `y = x - 0.95` and we are off to the races."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case where we have less than _N_ equations with _N_ variables, that gets into an area called _linear programming_. More about that in a later chapter."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Eigensomethingorother"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Something something _linear_, something something _eigen_\u2026 As the effects of the red pill begin to ramp up, let\u2019s take a good look at the notion of _eigenvalues_ and _eigenvectors_. Those seem to pop up all over linear algebra."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we have a square matrix **A**, a non-zero vector **x**, and some scalar \u03bb, such that **Ax** = \u03bb**x** holds. The vector **x** is called an _eigenvector_ of **A**. The scalar \u03bb is called an _eigenvalue_ of **A**. There may be multiple solutions for \u03bb and **x**, and we\u2019ll get to that. [Matthews](http://www.numbertheory.org/book/cha6.pdf) and [Strang](http://math.mit.edu/linearalgebra/ila0601.pdf) both provide lots of discussion about these matrix properties, albeit arriving in a roundabout way."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s take a more direct route:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Ax** = \u03bb**x**  \n",
      "**Ax** - \u03bb**x** = 0  \n",
      "**Ax** - \u03bb**Ix** = 0  \n",
      "(**A** - \u03bb**I**)**x** = 0  \n",
      "_det_(**A** - \u03bb**I**) = 0"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking at the second-to-last equation, remember that **x** must be non-zero. So we will be able to solve for the determinant."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider a matrix **A** defined as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>0</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>-2</td>\n",
      "\t\t\t<td>-3</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The corresponding \u03bb**I** is:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>\u03bb</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>0</td>\n",
      "\t\t\t<td>\u03bb</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then **A** - \u03bb**I** becomes:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>0 - \u03bb</td>\n",
      "\t\t\t<td>1 - 0</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>-2 - 0</td>\n",
      "\t\t\t<td>-3 - \u03bb</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The determinant _det_(**A** - \u03bb**I**) can be defined as \u03bb2 + 3\u03bb + 2, called the [characteristic polynomial](http://mathworld.wolfram.com/CharacteristicPolynomial.html) of **A**. Solving for the roots of that polynomial:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u03bb2 + 3\u03bb + 2 = 0  \n",
      "(\u03bb+ 1) \\* (\u03bb + 2) = 0  \n",
      "\u03bb1 = \u20131, \u03bb2 = \u20132"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the [linalg.eig](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig) method in NumPy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      ">>> import numpy as np\n",
      ">>> A = np.array([0, 1, -2, -3]).reshape(2, 2)\n",
      ">>> np.linalg.eig(A)\n",
      "(array([-1., -2.]), array([[0.70710678, -0.4472136],\n",
      "    [-0.70710678,  0.89442719]]))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In other words, the eigenvectors `[0.70710678, -0.4472136]` and `[-0.70710678, 0.89442719]` correspond to the eigenvalues \u03bb1 = \u20131 and \u03bb2 = \u20132 for matrix **A**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How do we use eigenvalues and eigenvectors? Oh, in so many ways! Let\u2019s start with the notion of matrix _decomposition_, for example used in [principal component analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) (PCA)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the [Breiman paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.4933&rep=rep1&type=pdf) in the previous chapter, where many input **x**\u2019s go into a \u201cblack box\u201d for algorithmic modeling. One issue that we grapple with continually in machine learning is _dimensional reduction_. In other words, modeling with 1000 different **x**\u2019s may become a problem \u2013 performance costs come to mind \u2013 while, say, 20 **v**\u2019s could work just fine."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use eigenvectors in the PCA calculations. Visualize a high-dimensional space, with all of those 1000 different **x**\u2019s \u2013 essentially looking at a \u201ccloud\u201d of data points that compares their _covariance_. We fit a straight line through that cloud of points, as best as possible. Oh, here\u2019s an idea\u2026 how about we use a _least squares approximation_ to solve that? Great. The resulting line defines an eigenvector, which we will call the first _principal component_ in this context. Elements of that eigenvector become the coefficients for a polynomial of the 1000 different **x**\u2019s \u2013 so we get an equation. That equation becomes one of the **v**\u2019s, as a kind of synthesized variable."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We peel off the eigenvector from the high-dimensional cloud that we\u2019ve been visualizing, then apply the least squares trick again. Great, we\u2019ve got another polynomial as a synthesized variable \u2013 as the second principal component. Rinse, lather, repeat. After 20 iterations we have 20 principal components. Each of those reduces the 1000 different **x**\u2019s (high dimension) into 20 different **v**\u2019s (low dimension). Ergo the name, dimensional reduction. There are many variants, and frankly [clustering algorithms](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) do pretty much the same trick, just in a different way."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Eigenvalues are also great for approximating derivatives (Calculus) and for methods to avoid costly matrix operations. For example, suppose you really needed to find the value of **A**N**x** for a large matrix? We can wager safely that finding \u03bb**x** then calculating \u03bbN**x** would be a whole lot quicker. In the context of large social graphs, as the matrix elements become numbered in the trillions, that performance issue becomes especially poignant."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Graph Theory"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s talk a bit about graphs\u2026 Graphs are composed of _vertices_ (the nodes in the graph) and _edges_ (the arcs between nodes). We can construct a special kind of matrix, called an _adjacency matrix_, which has a row for each vertex and a column for each vertex. The elements of an adjacency matrix have a `1` if an edge exists between the respective pair of vertices, and a `0` otherwise."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check out [An Introduction to Algebraic Graph Theory](http://buzzard.ups.edu/talks/beezer-2009-pacific-agt.pdf) by Rob Beezer, starting on slide 9. There are four vertices in the graph `[u, v, w, x]` and an adjacency matrix defined as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>0</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>1</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>0</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>1</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An adjacency matrix has certain properties. For example, we know that it is symmetric **A** = **A**T and that it has real eigenvalues."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the `eig` method in NumPy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      ">>> import numpy as np\n",
      ">>> A = np.array([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1]).reshape(4, 4)\n",
      ">>> A\n",
      "array([[0, 1, 0, 1],\n",
      "       [1, 0, 1, 1],\n",
      "       [0, 1, 0, 1],\n",
      "       [1, 1, 0, 1]])\n",
      "\n",
      ">>> np.linalg.eig(A)\n",
      "(array([2.65109341, -1.37720285, -0.27389055, 0.]),\n",
      "array([[-4.25648128e-01, -4.56381963e-01, -2.61003301e-01, 4.53246652e-17],\n",
      "    [-5.42229129e-01, 7.53529250e-01, -6.20457793e-01, -5.77350269e-01],\n",
      "    [-4.25648128e-01, -4.56381963e-01, -2.61003301e-01, -5.77350269e-01],\n",
      "    [-5.86203816e-01, -1.24998709e-01, 6.91944132e-01, 5.77350269e-01]]))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great, we can haz eigenvalues. Keep in mind that the eigenvalues do not define the graphs \u2013 different graphs could have the same eigenvalues. Even so, we can put those eigenvalues to good use in measuring graphs. For example, a [graph diameter](http://mathworld.wolfram.com/GraphDiameter.html) is defined as the \u201clongest shortest path\u201d \u2013 in other words, the max number of vertices which must be traversed to go from one vertex to another when paths do not backtrack. A graph of diameter `d` has `d + 1` distinct eigenvalues."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many more was to leverage eigenvalues, but the point for now is that we\u2019ve built bridges between abstract algebra and linear algebra, and then bridges between linear algebra and graph theory."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Sparsity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here\u2019s a tip: check out work by David Gleich, an assistant professor at Purdue \u2013 in particular, his [homepage](http://www.cs.purdue.edu/homes/dgleich/) and [many presentations on SlideShare](http://www.slideshare.net/dgleich/). If you take away merely two key points from **Just For Math**, one is to study what\u2019s going on in [Summingbird](https://github.com/twitter/summingbird/wiki) and the other is to keep an eye open for presentations by Gleich."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In particular, some of those [slide decks about PageRank](http://www.slideshare.net/dgleich/tag/pagerank) provide excellent examples of where linear algebra and graph theory intersect. [PageRank](http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PALL&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.htm&r=1&f=G&l=50&s1=6,285,999.PN.&OS=PN/6,285,999&RS=PN/6,285,999) is a graph algorithm at the core of Google search:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We\u2019ll go into more detail in a bit, but the point for now is that PageRank makes highly effective use of [eigenvalues and an adjacency matrix](http://en.wikipedia.org/wiki/PageRank#Power_Method) for its graph of links."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Back to the \u201cred pill\u201d and David Gleich\u2019s presentations, check out [What You Can Do With QR Factorization on Hadoop](http://www.slideshare.net/dgleich/whatyoucandowithtsqrfactorization). Assuming that one knows some Python, a bit about MapReduce, plus some linear algebra \u2013 which is a fair assumption for this audience \u2013 this presentation introduces the notion of _tall and skinny_ matrices, and how to leverage them. Tall and skinny is a way to describe a matrix that has many, many rows but not so many columns. For example, suppose you have a few million customers and maybe 100 data points on each \u2013 except that for many customers some of the data is missing. With one row per customer, that matrix becomes (1) tall and skinny and (2) sparse. What a great way to describe the data at oh so many many tech start-ups."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We probably want to leverage that data, and it\u2019s probably large enough that computation becomes difficult. Some example uses would be recommender systems, anti-fraud, advertising, etc. In that case, one approach is to use [QR factorization](http://en.wikipedia.org/wiki/QR_decomposition), which is another kind of decomposition. We say that a matrix **A** = **QR**, then we decompose **Q** and **R**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fortunately, the matrix **A** can be partitioned into chunks, so the problem can be parallelized \u2013 for example, on Hadoop. [Slides 13\u201330](http://www.slideshare.net/dgleich/whatyoucandowithtsqrfactorization) shows how the parallelized calculations work, using NumPy."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the benefits of this approach is that its results get appended together at the end \u2013 the chunks get reassembled without post-processing required. Often times in MapReduce apps, early stages can be parallelized, but the results get lumped into one big reduce at the end which takes forever to complete. QR factorization avoids that. There is a full [Python implementation](https://github.com/arbenson/mrtsqr) by Austin Benson @Stanford, and a [Scala implementation](https://github.com/ccsevers/scalding-linalg) by Chris Severs @eBay."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What are some use cases for QR factorization? Those include:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<ul>\n",
      "<li>regression when there are large samples</li>\n",
      "\t<li>\n",
      "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">principal component analysis</a> (PCA)</li>\n",
      "\t<li><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">K-means clustering</a></li>\n",
      "\t<li>\n",
      "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\">singular value decomposition</a> (SVD)</li>\n",
      "</ul>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u2026translated: image processing, recommender systems, anti-fraud, advertising, energy optimization, supply chain, agriculture, transportation, finance, etc. Just to name a few."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could keep going about different sparse matrix categories and their use cases, and how that greatly improves parallelization of important algorithms at scale. Check the slides. For now, let\u2019s close this section with a look at the [University of Florida Sparse Matrix Collection](http://www.cise.ufl.edu/research/sparse/matrices/), for example:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[ ![sparse matrix visualization: GHS_psdef/ford2, car surface mesh](http://www.research.att.com/~yifanhu/GALLERY/GRAPHS/GIF_SMALL/GHS_psdef@ford2.gif) ](http://www.cise.ufl.edu/research/sparse/matrices/GHS_psdef/ford2.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the curators, [Tim Davis](http://www.cise.ufl.edu/~davis/welcome.html), is a renowned authority on sparse matrix algorithms. As you begin to use some of the popular numerical packages for linear algebra, well, Davis wrote many of them. Not only is the _Sparse Matrix Collection_ beautiful to view, it\u2019s immensely practical as well: these become test cases for algorithm development and testing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Deconstructing Google Search"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s go back to the PageRank algorithm, taking a look at [The Mathematics of Google Search](http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html) by Raluca Tanase and Remus Radu @Cornell. Note their link graph, and also **A** as a transition matrix for that graph. This is similar to an adjacency matrix, except that the edges have weights. Those weights replace the `1` and `0` values for the matrix elements:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table border=\"1\"><tbody>\n",
      "<tr>\n",
      "<td>0</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t\t<td>1</td>\n",
      "\t\t\t<td>1/2</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>1/3</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>1/3</td>\n",
      "\t\t\t<td>1/2</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t\t<td>1/2</td>\n",
      "\t\t</tr>\n",
      "<tr>\n",
      "<td>1/3</td>\n",
      "\t\t\t<td>1/2</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t\t<td>0</td>\n",
      "\t\t</tr>\n",
      "</tbody></table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The way that PageRank works is to define an equation **A**N**v** = **v**, then find the eigenvector where the eigenvalue = 1, and that eigenvector represents the ranks for each web page. The max element in that eigenvector is the top-ranked page. We iterate _N_ times, until the system reaches equilibrium \u2013 in other words, until the values in the eigenvector stop changing appreciably."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is more to PageRank (e.g, the _damping factor_), and there are [more optimal ways](http://en.wikipedia.org/wiki/PageRank#Power_Method) to calculate PageRank, which in turn make even more use of eigenvalues. But the point is clear: linear algebra, FTW."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Graph Queries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One final note\u2026 [Titan](https://github.com/thinkaurelius/titan) is a graph query engine (and much, much more) running atop [Apache Cassandra](http://cassandra.apache.org/), [Apache HBase](http://hbase.apache.org/), [BerkeleyDB](http://www.oracle.com/technetwork/database/berkeleydb/overview/index-093405.html), etc. Hands down, this represents the most robust, optimal, and feature rich among the various open source graph query engines."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check out the [Getting Started with Titan](https://github.com/thinkaurelius/titan/wiki/Getting-Started) tutorial for a step-by-step introduction. That\u2019s one of the best introductions available for graph theory in practice."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Key Points"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**TODO**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<ul>\n",
      "<li>real-world problems: represented as graphs, then computed as sparse matrices, then leverage abstract algebra for parallelism</li>\n",
      "\t<li>linear algebra \u2013 e.g., calculating algorithms for large-scale apps efficiently</li>\n",
      "\t<li>graph theory \u2013 e.g., representation of problems in a calculable language</li>\n",
      "</ul>\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Suggested Books"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_Elementary Linear Algebra_  \n",
      "by Keith Matthews (1991)  \n",
      "http://www.numbertheory.org/book/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_Introduction to Linear Algebra_  \n",
      "by Gilbert Strang (2009)  \n",
      "http://www.amazon.com/dp/0980232716"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "_Algebraic Graph Theory_  \n",
      "by Norman Biggs (1974)  \n",
      "http://amazon.com/dp/0521458978"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exercises"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**TODO**"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}